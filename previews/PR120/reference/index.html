<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · RegularizedOptimization.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">RegularizedOptimization.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li class="is-active"><a class="tocitem" href>Reference</a><ul class="internal"><li><a class="tocitem" href="#Contents"><span>Contents</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/master/docs/src/reference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><h2 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h2><ul><li><a href="#Reference">Reference</a></li><ul><li><a href="#Contents">Contents</a></li><li><a href="#Index">Index</a></li></ul></ul><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#RegularizedOptimization.FISTA-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}"><code>RegularizedOptimization.FISTA</code></a></li><li><a href="#RegularizedOptimization.LM-Union{Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, ROSolverOptions}} where H"><code>RegularizedOptimization.LM</code></a></li><li><a href="#RegularizedOptimization.LMTR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, X, ROSolverOptions}} where {H, X}"><code>RegularizedOptimization.LMTR</code></a></li><li><a href="#RegularizedOptimization.PG-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}"><code>RegularizedOptimization.PG</code></a></li><li><a href="#RegularizedOptimization.R2-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}"><code>RegularizedOptimization.R2</code></a></li><li><a href="#RegularizedOptimization.TR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLPModel, H, X, ROSolverOptions}} where {H, X}"><code>RegularizedOptimization.TR</code></a></li><li><a href="#RegularizedOptimization.TRDH-Union{Tuple{R}, Tuple{NLPModels.AbstractNLPModel{R}, Any, Any, ROSolverOptions{R}}} where R&lt;:Real"><code>RegularizedOptimization.TRDH</code></a></li><li><a href="#RegularizedOptimization.prox_split_1w-NTuple{4, Any}"><code>RegularizedOptimization.prox_split_1w</code></a></li><li><a href="#RegularizedOptimization.prox_split_2w-NTuple{4, Any}"><code>RegularizedOptimization.prox_split_2w</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="RegularizedOptimization.FISTA-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}" href="#RegularizedOptimization.FISTA-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}"><code>RegularizedOptimization.FISTA</code></a> — <span class="docstring-category">Method</span></header><section><div><p>FISTA for   min_x ϕ(x) = f(x) + g(x), with f(x) cvx and β-smooth, g(x) closed cvx</p><p>Input:     f: function handle that returns f(x) and ∇f(x)     h: function handle that returns g(x)     s: initial point     proxG: function handle that calculates prox_{νg}     options: see descentopts.jl   Output:     s⁺: s update     s : s^(k-1)     his : function history     feval : number of function evals (total objective)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/ddbbba5ae8f801d856f293e805efdbe20e9025cf/src/Fista_alg.jl#L3-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RegularizedOptimization.LM-Union{Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, ROSolverOptions}} where H" href="#RegularizedOptimization.LM-Union{Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, ROSolverOptions}} where H"><code>RegularizedOptimization.LM</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">LM(nls, h, options; kwargs...)</code></pre><p>A Levenberg-Marquardt method for the problem</p><pre><code class="language-none">min ½ ‖F(x)‖² + h(x)</code></pre><p>where F: ℝⁿ → ℝᵐ and its Jacobian J are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.</p><p>At each iteration, a step s is computed as an approximate solution of</p><pre><code class="language-none">min  ½ ‖J(x) s + F(x)‖² + ½ σ ‖s‖² + ψ(s; x)</code></pre><p>where F(x) and J(x) are the residual and its Jacobian at x, respectively, ψ(s; x) = h(x + s), and σ &gt; 0 is a regularization parameter.</p><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel</code>: a smooth nonlinear least-squares problem</li><li><code>h</code>: a regularizer such as those defined in ProximalOperators</li><li><code>options::ROSolverOptions</code>: a structure containing algorithmic parameters</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x0::AbstractVector</code>: an initial guess (default: <code>nls.meta.x0</code>)</li><li><code>subsolver_logger::AbstractLogger</code>: a logger to pass to the subproblem solver</li><li><code>subsolver</code>: the procedure used to compute a step (<code>PG</code> or <code>R2</code>)</li><li><code>subsolver_options::ROSolverOptions</code>: default options to pass to the subsolver.</li><li><code>selected::AbstractVector{&lt;:Integer}</code>: (default <code>1:f.meta.nvar</code>).</li></ul><p><strong>Return values</strong></p><ul><li><code>xk</code>: the final iterate</li><li><code>Fobj_hist</code>: an array with the history of values of the smooth objective</li><li><code>Hobj_hist</code>: an array with the history of values of the nonsmooth objective</li><li><code>Complex_hist</code>: an array with the history of number of inner iterations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/ddbbba5ae8f801d856f293e805efdbe20e9025cf/src/LM_alg.jl#L3-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RegularizedOptimization.LMTR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, X, ROSolverOptions}} where {H, X}" href="#RegularizedOptimization.LMTR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, X, ROSolverOptions}} where {H, X}"><code>RegularizedOptimization.LMTR</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">LMTR(nls, h, χ, options; kwargs...)</code></pre><p>A trust-region Levenberg-Marquardt method for the problem</p><pre><code class="language-none">min ½ ‖F(x)‖² + h(x)</code></pre><p>where F: ℝⁿ → ℝᵐ and its Jacobian J are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous and proper.</p><p>At each iteration, a step s is computed as an approximate solution of</p><pre><code class="language-none">min  ½ ‖J(x) s + F(x)‖₂² + ψ(s; x)  subject to  ‖s‖ ≤ Δ</code></pre><p>where F(x) and J(x) are the residual and its Jacobian at x, respectively, ψ(s; x) = h(x + s), ‖⋅‖ is a user-defined norm and Δ &gt; 0 is a trust-region radius.</p><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel</code>: a smooth nonlinear least-squares problem</li><li><code>h</code>: a regularizer such as those defined in ProximalOperators</li><li><code>χ</code>: a norm used to define the trust region in the form of a regularizer</li><li><code>options::ROSolverOptions</code>: a structure containing algorithmic parameters</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x0::AbstractVector</code>: an initial guess (default: <code>nls.meta.x0</code>)</li><li><code>subsolver_logger::AbstractLogger</code>: a logger to pass to the subproblem solver</li><li><code>subsolver</code>: the procedure used to compute a step (<code>PG</code> or <code>R2</code>)</li><li><code>subsolver_options::ROSolverOptions</code>: default options to pass to the subsolver.</li><li><code>selected::AbstractVector{&lt;:Integer}</code>: (default <code>1:f.meta.nvar</code>).</li></ul><p><strong>Return values</strong></p><ul><li><code>xk</code>: the final iterate</li><li><code>Fobj_hist</code>: an array with the history of values of the smooth objective</li><li><code>Hobj_hist</code>: an array with the history of values of the nonsmooth objective</li><li><code>Complex_hist</code>: an array with the history of number of inner iterations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/ddbbba5ae8f801d856f293e805efdbe20e9025cf/src/LMTR_alg.jl#L3-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RegularizedOptimization.PG-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}" href="#RegularizedOptimization.PG-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}"><code>RegularizedOptimization.PG</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Proximal Gradient Descent  for</p><p>min_x ϕ(x) = f(x) + g(x), with f(x) β-smooth, g(x) closed, lsc</p><p>Input:   f: function handle that returns f(x) and ∇f(x)   h: function handle that returns g(x)   s: initial point   proxG: function handle that calculates prox_{νg}   options: see descentopts.jl Output:   s⁺: s update   s : s^(k-1)   his : function history   feval : number of function evals (total objective )</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/ddbbba5ae8f801d856f293e805efdbe20e9025cf/src/PG_alg.jl#L3-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RegularizedOptimization.R2-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}" href="#RegularizedOptimization.R2-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}"><code>RegularizedOptimization.R2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">R2(nlp, h, options)
R2(f, ∇f!, h, options, x0)</code></pre><p>A first-order quadratic regularization method for the problem</p><pre><code class="language-none">min f(x) + h(x)</code></pre><p>where f: ℝⁿ → ℝ has a Lipschitz-continuous gradient, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.</p><p>About each iterate xₖ, a step sₖ is computed as a solution of</p><pre><code class="language-none">min  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)</code></pre><p>where φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs is the Taylor linear approximation of f about xₖ, ψ(s; xₖ) = h(xₖ + s), ‖⋅‖ is a user-defined norm and σₖ &gt; 0 is the regularization parameter.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: a smooth optimization problem</li><li><code>h</code>: a regularizer such as those defined in ProximalOperators</li><li><code>options::ROSolverOptions</code>: a structure containing algorithmic parameters</li><li><code>x0::AbstractVector</code>: an initial guess (in the second calling form)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>x0::AbstractVector</code>: an initial guess (in the first calling form: default = <code>nlp.meta.x0</code>)</li><li><code>selected::AbstractVector{&lt;:Integer}</code>: (default <code>1:length(x0)</code>).</li></ul><p>The objective and gradient of <code>nlp</code> will be accessed.</p><p>In the second form, instead of <code>nlp</code>, the user may pass in</p><ul><li><code>f</code> a function such that <code>f(x)</code> returns the value of f at x</li><li><code>∇f!</code> a function to evaluate the gradient in place, i.e., such that <code>∇f!(g, x)</code> store ∇f(x) in <code>g</code>.</li></ul><p><strong>Return values</strong></p><ul><li><code>xk</code>: the final iterate</li><li><code>Fobj_hist</code>: an array with the history of values of the smooth objective</li><li><code>Hobj_hist</code>: an array with the history of values of the nonsmooth objective</li><li><code>Complex_hist</code>: an array with the history of number of inner iterations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/ddbbba5ae8f801d856f293e805efdbe20e9025cf/src/R2_alg.jl#L59-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RegularizedOptimization.TR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLPModel, H, X, ROSolverOptions}} where {H, X}" href="#RegularizedOptimization.TR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLPModel, H, X, ROSolverOptions}} where {H, X}"><code>RegularizedOptimization.TR</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TR(nlp, h, χ, options; kwargs...)</code></pre><p>A trust-region method for the problem</p><pre><code class="language-none">min f(x) + h(x)</code></pre><p>where f: ℝⁿ → ℝ has a Lipschitz-continuous Jacobian, and h: ℝⁿ → ℝ is lower semi-continuous and proper.</p><p>About each iterate xₖ, a step sₖ is computed as an approximate solution of</p><pre><code class="language-none">min  φ(s; xₖ) + ψ(s; xₖ)  subject to  ‖s‖ ≤ Δₖ</code></pre><p>where φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀ Bₖ s  is a quadratic approximation of f about xₖ, ψ(s; xₖ) = h(xₖ + s), ‖⋅‖ is a user-defined norm and Δₖ &gt; 0 is the trust-region radius. The subproblem is solved inexactly by way of a first-order method such as the proximal-gradient method or the quadratic regularization method.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: a smooth optimization problem</li><li><code>h</code>: a regularizer such as those defined in ProximalOperators</li><li><code>χ</code>: a norm used to define the trust region in the form of a regularizer</li><li><code>options::ROSolverOptions</code>: a structure containing algorithmic parameters</li></ul><p>The objective, gradient and Hessian of <code>nlp</code> will be accessed. The Hessian is accessed as an abstract operator and need not be the exact Hessian.</p><p><strong>Keyword arguments</strong></p><ul><li><code>x0::AbstractVector</code>: an initial guess (default: <code>nlp.meta.x0</code>)</li><li><code>subsolver_logger::AbstractLogger</code>: a logger to pass to the subproblem solver (default: the null logger)</li><li><code>subsolver</code>: the procedure used to compute a step (<code>PG</code> or <code>R2</code>)</li><li><code>subsolver_options::ROSolverOptions</code>: default options to pass to the subsolver (default: all defaut options)</li><li><code>selected::AbstractVector{&lt;:Integer}</code>: (default <code>1:f.meta.nvar</code>).</li></ul><p><strong>Return values</strong></p><ul><li><code>xk</code>: the final iterate</li><li><code>Fobj_hist</code>: an array with the history of values of the smooth objective</li><li><code>Hobj_hist</code>: an array with the history of values of the nonsmooth objective</li><li><code>Complex_hist</code>: an array with the history of number of inner iterations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/ddbbba5ae8f801d856f293e805efdbe20e9025cf/src/TR_alg.jl#L3-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RegularizedOptimization.TRDH-Union{Tuple{R}, Tuple{NLPModels.AbstractNLPModel{R}, Any, Any, ROSolverOptions{R}}} where R&lt;:Real" href="#RegularizedOptimization.TRDH-Union{Tuple{R}, Tuple{NLPModels.AbstractNLPModel{R}, Any, Any, ROSolverOptions{R}}} where R&lt;:Real"><code>RegularizedOptimization.TRDH</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TRDH(nlp, h, χ, options; kwargs...)
TRDH(f, ∇f!, h, options, x0)</code></pre><p>A trust-region method with diagonal Hessian approximation for the problem</p><pre><code class="language-none">min f(x) + h(x)</code></pre><p>where f: ℝⁿ → ℝ has a Lipschitz-continuous Jacobian, and h: ℝⁿ → ℝ is lower semi-continuous and proper.</p><p>About each iterate xₖ, a step sₖ is computed as an approximate solution of</p><pre><code class="language-none">min  φ(s; xₖ) + ψ(s; xₖ)  subject to  ‖s‖ ≤ Δₖ</code></pre><p>where φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀ Dₖ s  is a quadratic approximation of f about xₖ, ψ(s; xₖ) = h(xₖ + s), ‖⋅‖ is a user-defined norm, Dₖ is a diagonal Hessian approximation and Δₖ &gt; 0 is the trust-region radius.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: a smooth optimization problem</li><li><code>h</code>: a regularizer such as those defined in ProximalOperators</li><li><code>χ</code>: a norm used to define the trust region in the form of a regularizer</li><li><code>options::ROSolverOptions</code>: a structure containing algorithmic parameters</li></ul><p>The objective and gradient of <code>nlp</code> will be accessed.</p><p>In the second form, instead of <code>nlp</code>, the user may pass in</p><ul><li><code>f</code> a function such that <code>f(x)</code> returns the value of f at x</li><li><code>∇f!</code> a function to evaluate the gradient in place, i.e., such that <code>∇f!(g, x)</code> store ∇f(x) in <code>g</code></li><li><code>x0::AbstractVector</code>: an initial guess.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x0::AbstractVector</code>: an initial guess (default: <code>nlp.meta.x0</code>)</li><li><code>selected::AbstractVector{&lt;:Integer}</code>: (default <code>1:f.meta.nvar</code>)</li><li><code>Bk</code>: initial diagonal Hessian approximation (default: <code>(one(R) / options.ν) * I</code>).</li></ul><p><strong>Return values</strong></p><ul><li><code>xk</code>: the final iterate</li><li><code>Fobj_hist</code>: an array with the history of values of the smooth objective</li><li><code>Hobj_hist</code>: an array with the history of values of the nonsmooth objective</li><li><code>Complex_hist</code>: an array with the history of number of inner iterations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/ddbbba5ae8f801d856f293e805efdbe20e9025cf/src/TRDH_alg.jl#L3-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RegularizedOptimization.prox_split_1w-NTuple{4, Any}" href="#RegularizedOptimization.prox_split_1w-NTuple{4, Any}"><code>RegularizedOptimization.prox_split_1w</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Solves descent direction s for some objective function with the structure 	min<em>s q</em>k(s) + ψ(x+s) s.t. ||s||<em>q⩽ Δ 	for some Δ provided Arguments ––––– proxp : prox method for p-norm 	takes in z (vector), a (λ||⋅||</em>p), p is norm for ψ I think s0 : Vector{Float64,1} 	Initial guess for the descent direction projq : generic that projects onto ||⋅||<em>q⩽Δ norm ball options : mutable structure p</em>params</p><p><strong>Returns</strong></p><p>s   : Vector{Float64,1} 	Final value of Algorithm 6.1 descent direction w   : Vector{Float64,1} 	relaxation variable of Algorithm 6.1 descent direction</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/ddbbba5ae8f801d856f293e805efdbe20e9025cf/src/splitting.jl#L3-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RegularizedOptimization.prox_split_2w-NTuple{4, Any}" href="#RegularizedOptimization.prox_split_2w-NTuple{4, Any}"><code>RegularizedOptimization.prox_split_2w</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Solves descent direction s for some objective function with the structure 	min<em>s q</em>k(s) + ψ(x+s) s.t. ||s||<em>q⩽ Δ 	for some Δ provided Arguments ––––– proxp : prox method for p-norm 	takes in z (vector), a (λ||⋅||</em>p), p is norm for ψ I think s0 : Vector{Float64,1} 	Initial guess for the descent direction projq : generic that projects onto ||⋅||<em>q⩽Δ norm ball options : mutable structure p</em>params</p><p><strong>Returns</strong></p><p>s   : Vector{Float64,1} 	Final value of Algorithm 6.2 descent direction w   : Vector{Float64,1} 	relaxation variable of Algorithm 6.2 descent direction</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/ddbbba5ae8f801d856f293e805efdbe20e9025cf/src/splitting.jl#L96-L116">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tutorial/">« Tutorial</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 30 September 2023 00:12">Saturday 30 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
