<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · RegularizedOptimization.jl</title><meta name="title" content="Reference · RegularizedOptimization.jl"/><meta property="og:title" content="Reference · RegularizedOptimization.jl"/><meta property="twitter:title" content="Reference · RegularizedOptimization.jl"/><meta name="description" content="Documentation for RegularizedOptimization.jl."/><meta property="og:description" content="Documentation for RegularizedOptimization.jl."/><meta property="twitter:description" content="Documentation for RegularizedOptimization.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="RegularizedOptimization.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">RegularizedOptimization.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../algorithms/">Algorithms</a></li><li><a class="tocitem" href="../regularizers/">Regularizers</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/basic/">A regularized optimization problem</a></li><li><a class="tocitem" href="../examples/ls/">A regularized least-square problem</a></li><li><a class="tocitem" href="../examples/custom_regularizer/">Custom regularizers</a></li></ul></li><li class="is-active"><a class="tocitem" href>Reference</a><ul class="internal"><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li><li><a class="tocitem" href="../bibliography/">Bibliography</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/master/docs/src/reference.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#RegularizedOptimization.ALSolver"><code>RegularizedOptimization.ALSolver</code></a></li><li><a href="#RegularizedOptimization.LMModel"><code>RegularizedOptimization.LMModel</code></a></li><li><a href="#RegularizedOptimization.R2NModel"><code>RegularizedOptimization.R2NModel</code></a></li><li><a href="#RegularizedOptimization.AL-Tuple{Val{:equ}, RegularizedProblems.AbstractRegularizedNLPModel}"><code>RegularizedOptimization.AL</code></a></li><li><a href="#RegularizedOptimization.LM-Union{Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, ROSolverOptions}} where H"><code>RegularizedOptimization.LM</code></a></li><li><a href="#RegularizedOptimization.LMTR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, X, ROSolverOptions}} where {H, X}"><code>RegularizedOptimization.LMTR</code></a></li><li><a href="#RegularizedOptimization.R2-Union{Tuple{V}, Tuple{R}, Tuple{NLPModels.AbstractNLPModel{R, V}, Any, ROSolverOptions{R}}} where {R&lt;:Real, V}"><code>RegularizedOptimization.R2</code></a></li><li><a href="#RegularizedOptimization.R2DH-Union{Tuple{V}, Tuple{T}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{T, V}, Any, ROSolverOptions{T}}} where {T, V}"><code>RegularizedOptimization.R2DH</code></a></li><li><a href="#RegularizedOptimization.R2N-Union{Tuple{V}, Tuple{T}, Tuple{NLPModels.AbstractNLPModel{T, V}, Any, ROSolverOptions{T}}} where {T&lt;:Real, V}"><code>RegularizedOptimization.R2N</code></a></li><li><a href="#RegularizedOptimization.RegularizedExecutionStats-Union{Tuple{RegularizedProblems.AbstractRegularizedNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>RegularizedOptimization.RegularizedExecutionStats</code></a></li><li><a href="#RegularizedOptimization.TR-Union{Tuple{R}, Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLPModel, H, X, ROSolverOptions{R}}} where {H, X, R}"><code>RegularizedOptimization.TR</code></a></li><li><a href="#RegularizedOptimization.TRDH-Union{Tuple{V}, Tuple{T}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{T, V}, Any, Any, ROSolverOptions{T}}} where {T, V}"><code>RegularizedOptimization.TRDH</code></a></li><li><a href="#RegularizedOptimization.project_y!-Tuple{Percival.AugLagModel}"><code>RegularizedOptimization.project_y!</code></a></li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.ALSolver" href="#RegularizedOptimization.ALSolver"><code>RegularizedOptimization.ALSolver</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AL(reg_nlp; kwargs...)</code></pre><p>An augmented Lagrangian method for constrained regularized optimization, namely problems in the form</p><pre><code class="nohighlight hljs">minimize    f(x) + h(x)
subject to  lvar ≤ x ≤ uvar,
            lcon ≤ c(x) ≤ ucon</code></pre><p>where f: ℝⁿ → ℝ, c: ℝⁿ → ℝᵐ and their derivatives are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.</p><p>At each iteration, an iterate x is computed as an approximate solution of the subproblem</p><pre><code class="nohighlight hljs">minimize    L(x;y,μ) + h(x)
subject to  lvar ≤ x ≤ uvar</code></pre><p>where y is an estimate of the Lagrange multiplier vector for the constraints lcon ≤ c(x) ≤ ucon,  μ is the penalty parameter and L(⋅;y,μ) is the augmented Lagrangian function defined by</p><pre><code class="nohighlight hljs">L(x;y,μ) := f(x) - yᵀc(x) + ½ μ ‖c(x)‖².</code></pre><p>For advanced usage, first define a solver &quot;ALSolver&quot; to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = ALSolver(reg_nlp)
solve!(solver, reg_nlp)

stats = GenericExecutionStats(reg_nlp.model)
solver = ALSolver(reg_nlp)
solve!(solver, reg_nlp, stats)</code></pre><p><strong>Arguments</strong></p><ul><li><code>reg_nlp::AbstractRegularizedNLPModel</code>: a regularized optimization problem, see <code>RegularizedProblems.jl</code>,  consisting of <code>model</code> representing a smooth optimization problem, see <code>NLPModels.jl</code>, and a regularizer <code>h</code> such as those defined in <code>ProximalOperators.jl</code>.</li></ul><p>The objective and gradient of <code>model</code> will be accessed. The Hessian of <code>model</code> may be accessed or not, depending on the subsolver adopted. If adopted, the Hessian is accessed as an abstract operator and need not be the exact Hessian.</p><p><strong>Keyword arguments</strong></p><ul><li><code>x::AbstractVector</code>: a primal initial guess (default: <code>reg_nlp.model.meta.x0</code>)</li><li><code>y::AbstractVector</code>: a dual initial guess (default: <code>reg_nlp.model.meta.y0</code>)</li><li><code>atol::T = √eps(T)</code>: absolute optimality tolerance;</li><li><code>ctol::T = atol</code>: absolute feasibility tolerance;</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration;</li><li><code>max_iter::Int = 10000</code>: maximum number of iterations;</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds;</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function (negative number means unlimited);</li><li><code>subsolver::AbstractOptimizationSolver = has_bounds(nlp) ? TR : R2</code>: the procedure used to compute a step (e.g. <code>PG</code>, <code>R2</code>, <code>TR</code> or <code>TRDH</code>);</li><li><code>subsolver_logger::AbstractLogger</code>: a logger to pass to the subproblem solver;</li><li><code>init_penalty::T = T(10)</code>: initial penalty parameter;</li><li><code>factor_penalty_up::T = T(2)</code>: multiplicative factor to increase the penalty parameter;</li><li><code>factor_primal_linear_improvement::T = T(3/4)</code>: fraction to declare sufficient improvement of feasibility;</li><li><code>init_subtol::T = T(0.1)</code>: initial subproblem tolerance;</li><li><code>factor_decrease_subtol::T = T(1/4)</code>: multiplicative factor to decrease the subproblem tolerance;</li><li><code>dual_safeguard = (nlp::AugLagModel) -&gt; nothing</code>: in-place function to modify, as needed, the dual estimate.</li></ul><p><strong>Output</strong></p><ul><li><code>stats::GenericExecutionStats</code>: solution and other info, see <code>SolverCore.jl</code>.</li></ul><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>reg_nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.y</code>: current dual estimate;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention;</li><li><code>stats.elapsed_time</code>: elapsed time in seconds;</li><li><code>stats.solver_specific[:smooth_obj]</code>: current value of the smooth part of the objective function;</li><li><code>stats.solver_specific[:nonsmooth_obj]</code>: current value of the nonsmooth part of the objective function.</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/AL_alg.jl#L60-L140">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.LMModel" href="#RegularizedOptimization.LMModel"><code>RegularizedOptimization.LMModel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LMModel(j_prod!, jt_prod, F, v, σ, xk)</code></pre><p>Given the unconstrained optimization problem:</p><p class="math-container">\[\min \tfrac{1}{2} \| F(x) \|^2,\]</p><p>this model represents the smooth LM subproblem:</p><p class="math-container">\[\min_s \ \tfrac{1}{2} \| F(x) + J(x)s \|^2 + \tfrac{1}{2} σ \|s\|^2\]</p><p>where <code>J</code> is the Jacobian of <code>F</code> at <code>xk</code>, represented via matrix-free operations. <code>j_prod!(xk, s, out)</code> computes <code>J(xk) * s</code>, and <code>jt_prod!(xk, r, out)</code> computes <code>J(xk)&#39; * r</code>.</p><p><code>σ &gt; 0</code> is a regularization parameter and <code>v</code> is a vector of the same size as <code>F(xk)</code> used for intermediary computations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/LMModel.jl#L3-L18">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.R2NModel" href="#RegularizedOptimization.R2NModel"><code>RegularizedOptimization.R2NModel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">R2NModel(B, ∇f, v, σ, x0)</code></pre><p>Given the unconstrained optimization problem:</p><p class="math-container">\[\min f(x),\]</p><p>this model represents the smooth R2N subproblem:</p><p class="math-container">\[\min_s \ ∇f^T s + \tfrac{1}{2} s^T B s + \tfrac{1}{2} σ \|s\|^2\]</p><p>where <code>B</code> is either an approximation of the Hessian of <code>f</code> or the Hessian itself and <code>∇f</code> represents the gradient of <code>f</code> at <code>x0</code>. <code>σ &gt; 0</code> is a regularization parameter and <code>v</code> is a vector of the same size as <code>x0</code> used for intermediary computations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/R2NModel.jl#L3-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.AL-Tuple{Val{:equ}, RegularizedProblems.AbstractRegularizedNLPModel}" href="#RegularizedOptimization.AL-Tuple{Val{:equ}, RegularizedProblems.AbstractRegularizedNLPModel}"><code>RegularizedOptimization.AL</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AL(reg_nlp; kwargs...)</code></pre><p>An augmented Lagrangian method for constrained regularized optimization, namely problems in the form</p><pre><code class="nohighlight hljs">minimize    f(x) + h(x)
subject to  lvar ≤ x ≤ uvar,
            lcon ≤ c(x) ≤ ucon</code></pre><p>where f: ℝⁿ → ℝ, c: ℝⁿ → ℝᵐ and their derivatives are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.</p><p>At each iteration, an iterate x is computed as an approximate solution of the subproblem</p><pre><code class="nohighlight hljs">minimize    L(x;y,μ) + h(x)
subject to  lvar ≤ x ≤ uvar</code></pre><p>where y is an estimate of the Lagrange multiplier vector for the constraints lcon ≤ c(x) ≤ ucon,  μ is the penalty parameter and L(⋅;y,μ) is the augmented Lagrangian function defined by</p><pre><code class="nohighlight hljs">L(x;y,μ) := f(x) - yᵀc(x) + ½ μ ‖c(x)‖².</code></pre><p>For advanced usage, first define a solver &quot;ALSolver&quot; to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = ALSolver(reg_nlp)
solve!(solver, reg_nlp)

stats = GenericExecutionStats(reg_nlp.model)
solver = ALSolver(reg_nlp)
solve!(solver, reg_nlp, stats)</code></pre><p><strong>Arguments</strong></p><ul><li><code>reg_nlp::AbstractRegularizedNLPModel</code>: a regularized optimization problem, see <code>RegularizedProblems.jl</code>,  consisting of <code>model</code> representing a smooth optimization problem, see <code>NLPModels.jl</code>, and a regularizer <code>h</code> such as those defined in <code>ProximalOperators.jl</code>.</li></ul><p>The objective and gradient of <code>model</code> will be accessed. The Hessian of <code>model</code> may be accessed or not, depending on the subsolver adopted. If adopted, the Hessian is accessed as an abstract operator and need not be the exact Hessian.</p><p><strong>Keyword arguments</strong></p><ul><li><code>x::AbstractVector</code>: a primal initial guess (default: <code>reg_nlp.model.meta.x0</code>)</li><li><code>y::AbstractVector</code>: a dual initial guess (default: <code>reg_nlp.model.meta.y0</code>)</li><li><code>atol::T = √eps(T)</code>: absolute optimality tolerance;</li><li><code>ctol::T = atol</code>: absolute feasibility tolerance;</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration;</li><li><code>max_iter::Int = 10000</code>: maximum number of iterations;</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds;</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function (negative number means unlimited);</li><li><code>subsolver::AbstractOptimizationSolver = has_bounds(nlp) ? TR : R2</code>: the procedure used to compute a step (e.g. <code>PG</code>, <code>R2</code>, <code>TR</code> or <code>TRDH</code>);</li><li><code>subsolver_logger::AbstractLogger</code>: a logger to pass to the subproblem solver;</li><li><code>init_penalty::T = T(10)</code>: initial penalty parameter;</li><li><code>factor_penalty_up::T = T(2)</code>: multiplicative factor to increase the penalty parameter;</li><li><code>factor_primal_linear_improvement::T = T(3/4)</code>: fraction to declare sufficient improvement of feasibility;</li><li><code>init_subtol::T = T(0.1)</code>: initial subproblem tolerance;</li><li><code>factor_decrease_subtol::T = T(1/4)</code>: multiplicative factor to decrease the subproblem tolerance;</li><li><code>dual_safeguard = (nlp::AugLagModel) -&gt; nothing</code>: in-place function to modify, as needed, the dual estimate.</li></ul><p><strong>Output</strong></p><ul><li><code>stats::GenericExecutionStats</code>: solution and other info, see <code>SolverCore.jl</code>.</li></ul><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>reg_nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.y</code>: current dual estimate;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention;</li><li><code>stats.elapsed_time</code>: elapsed time in seconds;</li><li><code>stats.solver_specific[:smooth_obj]</code>: current value of the smooth part of the objective function;</li><li><code>stats.solver_specific[:nonsmooth_obj]</code>: current value of the nonsmooth part of the objective function.</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/AL_alg.jl#L60-L140">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.LM-Union{Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, ROSolverOptions}} where H" href="#RegularizedOptimization.LM-Union{Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, ROSolverOptions}} where H"><code>RegularizedOptimization.LM</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LM(reg_nls; kwargs...)</code></pre><p>A Levenberg-Marquardt method for the problem</p><pre><code class="nohighlight hljs">min ½ ‖F(x)‖² + h(x)</code></pre><p>where F: ℝⁿ → ℝᵐ and its Jacobian J are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.</p><p>At each iteration, a step s is computed as an approximate solution of</p><pre><code class="nohighlight hljs">min  ½ ‖J(x) s + F(x)‖² + ½ σ ‖s‖² + ψ(s; x)</code></pre><p>where F(x) and J(x) are the residual and its Jacobian at x, respectively, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is the ℓ₂ norm and σₖ &gt; 0 is the regularization parameter.</p><p>For advanced usage, first define a solver &quot;LMSolver&quot; to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = LMSolver(reg_nls; subsolver = R2Solver, m_monotone = 1)
solve!(solver, reg_nls)

stats = RegularizedExecutionStats(reg_nls)
solve!(solver, reg_nls, stats)</code></pre><p><strong>Arguments</strong></p><ul><li><code>reg_nls::AbstractRegularizedNLPModel{T, V}</code>: the problem to solve, see <code>RegularizedProblems.jl</code>, <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess;</li><li><code>nonlinear::Bool = true</code>: whether the function <code>F</code> is nonlinear or not.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance;</li><li><code>rtol::T = √eps(T)</code>: relative tolerance;</li><li>`neg_tol::T = zero(T): negative tolerance;</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function (negative number means unlimited);</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds;</li><li><code>max_iter::Int = 10000</code>: maximum number of iterations;</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration;</li><li><code>σmin::T = eps(T)</code>: minimum value of the regularization parameter;</li><li><code>σk::T = eps(T)^(1 / 5)</code>: initial value of the regularization parameter;</li><li><code>η1::T = √√eps(T)</code>: successful iteration threshold;</li><li><code>η2::T = T(0.9)</code>: very successful iteration threshold;</li><li><code>γ::T = T(3)</code>: regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful;</li><li><code>θ::T = 1/(1 + eps(T)^(1 / 5))</code>: is the model decrease fraction with respect to the decrease of the Cauchy model;</li><li><code>m_monotone::Int = 1</code>: monotonicity parameter. By default, LM is monotone but the non-monotone variant will be used if <code>m_monotone &gt; 1</code>;</li><li><code>subsolver = R2Solver</code>: the solver used to solve the subproblems.</li></ul><p>The algorithm stops either when <code>√(ξₖ/νₖ) &lt; atol + rtol*√(ξ₀/ν₀)</code> or <code>ξₖ &lt; 0</code> and <code>√(-ξₖ/νₖ) &lt; neg_tol</code> where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.xk</code>: current iterate;</li><li><code>solver.∇fk</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.solver_specific[:smooth_obj]</code>: current value of the smooth part of the objective function;</li><li><code>stats.solver_specific[:nonsmooth_obj]</code>: current value of the nonsmooth part of the objective function;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything other than <code>:unknown</code> will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention;</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/LM_alg.jl#L97-L151">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.LMTR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, X, ROSolverOptions}} where {H, X}" href="#RegularizedOptimization.LMTR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, X, ROSolverOptions}} where {H, X}"><code>RegularizedOptimization.LMTR</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LMTR(reg_nls; kwargs...)
LMTR(nls, h, χ, options; kwargs...)</code></pre><p>A trust-region Levenberg-Marquardt method for the problem</p><pre><code class="nohighlight hljs">min ½ ‖F(x)‖² + h(x)</code></pre><p>where F: ℝⁿ → ℝᵐ and its Jacobian J are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous and proper.</p><p>At each iteration, a step s is computed as an approximate solution of</p><pre><code class="nohighlight hljs">min  ½ ‖J(x) s + F(x)‖₂² + ψ(s; x)  subject to  ‖s‖ ≤ Δ</code></pre><p>where F(x) and J(x) are the residual and its Jacobian at x, respectively, ψ(s; x) = h(x + s), ‖⋅‖ is a user-defined norm and Δ &gt; 0 is a trust-region radius.</p><p>For advanced usage, first define a solver &quot;LMSolver&quot; to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = LMTRSolver(reg_nls; χ = NormLinf(one(T)), subsolver = R2Solver)
solve!(solver, reg_nls)

stats = RegularizedExecutionStats(reg_nls)
solve!(solver, reg_nls, stats)</code></pre><p><strong>Arguments</strong></p><ul><li><code>reg_nls::AbstractRegularizedNLPModel{T, V}</code>: the problem to solve, see <code>RegularizedProblems.jl</code>, <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess;</li><li><code>atol::T = √eps(T)</code>: absolute tolerance;</li><li><code>sub_atol::T = atol</code>: subsolver absolute tolerance;</li><li><code>rtol::T = √eps(T)</code>: relative tolerance;</li><li>`neg_tol::T = zero(T): negative tolerance;</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function (negative number means unlimited);</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds;</li><li><code>max_iter::Int = 10000</code>: maximum number of iterations;</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration;</li><li><code>Δk::T = eps(T)</code>: initial value of the trust-region radius;</li><li><code>η1::T = √√eps(T)</code>: successful iteration threshold;</li><li><code>η2::T = T(0.9)</code>: very successful iteration threshold;</li><li><code>γ::T = T(3)</code>: trust-region radius parameter multiplier, Δ := Δ*γ when the iteration is very successful and Δ := Δ/γ when the iteration is unsuccessful;</li><li><code>α::T = 1/eps(T)</code>: TODO</li><li><code>β::T = 1/eps(T)</code>: TODO</li><li><code>χ =  NormLinf(1)</code>: norm used to define the trust-region;`</li><li><code>subsolver::S = R2Solver</code>: subsolver used to solve the subproblem that appears at each iteration.</li></ul><p>The algorithm stops either when <code>√(ξₖ/νₖ) &lt; atol + rtol*√(ξ₀/ν₀)</code> or <code>ξₖ &lt; 0</code> and <code>√(-ξₖ/νₖ) &lt; neg_tol</code> where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.xk</code>: current iterate;</li><li><code>solver.∇fk</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.solver_specific[:smooth_obj]</code>: current value of the smooth part of the objective function;</li><li><code>stats.solver_specific[:nonsmooth_obj]</code>: current value of the nonsmooth part of the objective function;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything other than <code>:unknown</code> will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention;</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/LMTR_alg.jl#L96-L151">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.R2-Union{Tuple{V}, Tuple{R}, Tuple{NLPModels.AbstractNLPModel{R, V}, Any, ROSolverOptions{R}}} where {R&lt;:Real, V}" href="#RegularizedOptimization.R2-Union{Tuple{V}, Tuple{R}, Tuple{NLPModels.AbstractNLPModel{R, V}, Any, ROSolverOptions{R}}} where {R&lt;:Real, V}"><code>RegularizedOptimization.R2</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">R2(reg_nlp; kwargs…)</code></pre><p>A first-order quadratic regularization method for the problem</p><pre><code class="nohighlight hljs">min f(x) + h(x)</code></pre><p>where f: ℝⁿ → ℝ has a Lipschitz-continuous gradient, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.</p><p>About each iterate xₖ, a step sₖ is computed as a solution of</p><pre><code class="nohighlight hljs">min  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)</code></pre><p>where φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs is the Taylor linear approximation of f about xₖ, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is a user-defined norm and σₖ &gt; 0 is the regularization parameter.</p><p>For advanced usage, first define a solver &quot;R2Solver&quot; to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = R2Solver(reg_nlp)
solve!(solver, reg_nlp)

stats = RegularizedExecutionStats(reg_nlp)
solver = R2Solver(reg_nlp)
solve!(solver, reg_nlp, stats)</code></pre><p><strong>Arguments</strong></p><ul><li><code>reg_nlp::AbstractRegularizedNLPModel{T, V}</code>: the problem to solve, see <code>RegularizedProblems.jl</code>, <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess;</li><li><code>atol::T = √eps(T)</code>: absolute tolerance;</li><li><code>rtol::T = √eps(T)</code>: relative tolerance;</li><li><code>neg_tol::T = eps(T)^(1 / 4)</code>: negative tolerance</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function (negative number means unlimited);</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds;</li><li><code>max_iter::Int = 10000</code>: maximum number of iterations;</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration;</li><li><code>σmin::T = eps(T)</code>: minimum value of the regularization parameter;</li><li><code>η1::T = √√eps(T)</code>: very successful iteration threshold;</li><li><code>η2::T = T(0.9)</code>: successful iteration threshold;</li><li><code>ν::T = eps(T)^(1 / 5)</code>: multiplicative inverse of the regularization parameter: ν = 1/σ;</li><li><code>γ::T = T(3)</code>: regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful.</li></ul><p>The algorithm stops either when <code>√(ξₖ/νₖ) &lt; atol + rtol*√(ξ₀/ν₀)</code> or <code>ξₖ &lt; 0</code> and <code>√(-ξₖ/νₖ) &lt; neg_tol</code> where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.xk</code>: current iterate;</li><li><code>solver.∇fk</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.solver_specific[:smooth_obj]</code>: current value of the smooth part of the objective function;</li><li><code>stats.solver_specific[:nonsmooth_obj]</code>: current value of the nonsmooth part of the objective function;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything other than <code>:unknown</code> will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention;</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/R2_alg.jl#L113-L164">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.R2DH-Union{Tuple{V}, Tuple{T}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{T, V}, Any, ROSolverOptions{T}}} where {T, V}" href="#RegularizedOptimization.R2DH-Union{Tuple{V}, Tuple{T}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{T, V}, Any, ROSolverOptions{T}}} where {T, V}"><code>RegularizedOptimization.R2DH</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">R2DH(reg_nlp; kwargs…)</code></pre><p>A second-order quadratic regularization method for the problem</p><pre><code class="nohighlight hljs">min f(x) + h(x)</code></pre><p>where f: ℝⁿ → ℝ is C¹, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.</p><p>About each iterate xₖ, a step sₖ is computed as a solution of</p><pre><code class="nohighlight hljs">min  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)</code></pre><p>where φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀDₖs is a diagonal quadratic approximation of f about xₖ, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is the ℓ₂ norm and σₖ &gt; 0 is the regularization parameter.</p><p>For advanced usage, first define a solver <code>R2DHSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = R2DHSolver(reg_nlp; m_monotone = 6)
solve!(solver, reg_nlp)</code></pre><p>or</p><pre><code class="nohighlight hljs">stats = RegularizedExecutionStats(reg_nlp)
solver = R2DHSolver(reg_nlp)
solve!(solver, reg_nlp, stats)</code></pre><p><strong>Arguments</strong></p><ul><li><code>reg_nlp::AbstractRegularizedNLPModel{T, V}</code>: the problem to solve, see <code>RegularizedProblems.jl</code>, <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess;</li><li><code>atol::T = √eps(T)</code>: absolute tolerance;</li><li><code>rtol::T = √eps(T)</code>: relative tolerance;</li><li><code>neg_tol::T = eps(T)^(1 / 4)</code>: negative tolerance</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function (negative number means unlimited);</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds;</li><li><code>max_iter::Int = 10000</code>: maximum number of iterations;</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration;</li><li><code>σmin::T = eps(T)</code>: minimum value of the regularization parameter;</li><li><code>σk::T = eps(T)^(1 / 5)</code>: initial value of the regularization parameter;</li><li><code>η1::T = √√eps(T)</code>: very successful iteration threshold;</li><li><code>η2::T = T(0.9)</code>: successful iteration threshold;</li><li><code>γ::T = T(3)</code>: regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful.</li><li><code>θ::T = 1/(1 + eps(T)^(1 / 5))</code>: is the model decrease fraction with respect to the decrease of the Cauchy model. </li><li><code>m_monotone::Int = 6</code>: monotoneness parameter. By default, R2DH is non-monotone but the monotone variant can be used with <code>m_monotone = 1</code></li></ul><p>The algorithm stops either when <code>√(ξₖ/νₖ) &lt; atol + rtol*√(ξ₀/ν₀)</code> or <code>ξₖ &lt; 0</code> and <code>√(-ξₖ/νₖ) &lt; neg_tol</code> where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.xk</code>: current iterate;</li><li><code>solver.∇fk</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.solver_specific[:smooth_obj]</code>: current value of the smooth part of the objective function;</li><li><code>stats.solver_specific[:nonsmooth_obj]</code>: current value of the nonsmooth part of the objective function;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything other than <code>:unknown</code> will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention;</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/R2DH.jl#L84-L138">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.R2N-Union{Tuple{V}, Tuple{T}, Tuple{NLPModels.AbstractNLPModel{T, V}, Any, ROSolverOptions{T}}} where {T&lt;:Real, V}" href="#RegularizedOptimization.R2N-Union{Tuple{V}, Tuple{T}, Tuple{NLPModels.AbstractNLPModel{T, V}, Any, ROSolverOptions{T}}} where {T&lt;:Real, V}"><code>RegularizedOptimization.R2N</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">R2N(reg_nlp; kwargs…)</code></pre><p>A second-order quadratic regularization method for the problem</p><pre><code class="nohighlight hljs">min f(x) + h(x)</code></pre><p>where f: ℝⁿ → ℝ is C¹, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.</p><p>About each iterate xₖ, a step sₖ is computed as a solution of</p><pre><code class="nohighlight hljs">min  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)</code></pre><p>where φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀBₖs is a quadratic approximation of f about xₖ, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is the ℓ₂ norm and σₖ &gt; 0 is the regularization parameter.</p><p>For advanced usage, first define a solver &quot;R2NSolver&quot; to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = R2NSolver(reg_nlp; m_monotone = 1)
solve!(solver, reg_nlp)

stats = RegularizedExecutionStats(reg_nlp)
solve!(solver, reg_nlp, stats)</code></pre><p><strong>Arguments</strong></p><ul><li><code>reg_nlp::AbstractRegularizedNLPModel{T, V}</code>: the problem to solve, see <code>RegularizedProblems.jl</code>, <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess;</li><li><code>atol::T = √eps(T)</code>: absolute tolerance;</li><li><code>rtol::T = √eps(T)</code>: relative tolerance;</li><li><code>neg_tol::T = eps(T)^(1 / 4)</code>: negative tolerance;</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function (negative number means unlimited);</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds;</li><li><code>max_iter::Int = 10000</code>: maximum number of iterations;</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration;</li><li><code>σmin::T = eps(T)</code>: minimum value of the regularization parameter;</li><li><code>σk::T = eps(T)^(1 / 5)</code>: initial value of the regularization parameter;</li><li><code>η1::T = √√eps(T)</code>: successful iteration threshold;</li><li><code>η2::T = T(0.9)</code>: very successful iteration threshold;</li><li><code>γ::T = T(3)</code>: regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful;</li><li><code>θ::T = 1/(1 + eps(T)^(1 / 5))</code>: is the model decrease fraction with respect to the decrease of the Cauchy model;</li><li><code>opnorm_maxiter::Int = 5</code>: how many iterations of the power method to use to compute the operator norm of Bₖ. If a negative number is provided, then Arpack is used instead;</li><li><code>m_monotone::Int = 1</code>: monotonicity parameter. By default, R2N is monotone but the non-monotone variant will be used if <code>m_monotone &gt; 1</code>;</li><li><code>sub_kwargs::NamedTuple = NamedTuple()</code>: a named tuple containing the keyword arguments to be sent to the subsolver. The solver will fail if invalid keyword arguments are provided to the subsolver. For example, if the subsolver is <code>R2Solver</code>, you can pass <code>sub_kwargs = (max_iter = 100, σmin = 1e-6,)</code>.</li></ul><p>The algorithm stops either when <code>√(ξₖ/νₖ) &lt; atol + rtol*√(ξ₀/ν₀)</code> or <code>ξₖ &lt; 0</code> and <code>√(-ξₖ/νₖ) &lt; neg_tol</code> where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.xk</code>: current iterate;</li><li><code>solver.∇fk</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.solver_specific[:smooth_obj]</code>: current value of the smooth part of the objective function;</li><li><code>stats.solver_specific[:nonsmooth_obj]</code>: current value of the nonsmooth part of the objective function;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything other than <code>:unknown</code> will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention;</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>Similarly to the callback, when using a quasi-Newton approximation, two functions, <code>qn_update_y!(nlp, solver, stats)</code> and <code>qn_copy!(nlp, solver, stats)</code> are called at each update of the approximation. Namely, the former computes the <code>y</code> vector for which the pair <code>(s, y)</code> is pushed into the approximation. By default, <code>y := ∇fk⁻ - ∇fk</code>. The latter allows the user to tell which values should be copied for the next iteration. By default, only the gradient is copied: <code>∇fk⁻ .= ∇fk</code>. This might be useful when using R2N in a constrained optimization context, when the gradient of the Lagrangian function is pushed at each iteration rather than the gradient of the objective function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/R2N.jl#L99-L159">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.RegularizedExecutionStats-Union{Tuple{RegularizedProblems.AbstractRegularizedNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}" href="#RegularizedOptimization.RegularizedExecutionStats-Union{Tuple{RegularizedProblems.AbstractRegularizedNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>RegularizedOptimization.RegularizedExecutionStats</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GenericExecutionStats(reg_nlp :: AbstractRegularizedNLPModel{T, V})</code></pre><p>Construct a GenericExecutionStats object from an AbstractRegularizedNLPModel.  More specifically, construct a GenericExecutionStats on the NLPModel of reg<em>nlp and add three solver</em>specific entries namely :smooth<em>obj, :nonsmooth</em>obj and :xi. This is useful for reducing the number of allocations when calling solve!(..., reg<em>nlp, stats) and should be used by default. Warning: This should <em>not</em> be used when adding other solver</em>specific entries that do not have the current scalar type. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/utils.jl#L115-L122">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.TR-Union{Tuple{R}, Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLPModel, H, X, ROSolverOptions{R}}} where {H, X, R}" href="#RegularizedOptimization.TR-Union{Tuple{R}, Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLPModel, H, X, ROSolverOptions{R}}} where {H, X, R}"><code>RegularizedOptimization.TR</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TR(reg_nlp; kwargs…)
TR(nlp, h, χ, options; kwargs...)</code></pre><p>A trust-region method for the problem</p><pre><code class="nohighlight hljs">min f(x) + h(x)</code></pre><p>where f: ℝⁿ → ℝ has a Lipschitz-continuous gradient, and h: ℝⁿ → ℝ is lower semi-continuous and proper.</p><p>About each iterate xₖ, a step sₖ is computed as an approximate solution of</p><pre><code class="nohighlight hljs">min  φ(s; xₖ) + ψ(s; xₖ)  subject to  ‖s‖ ≤ Δₖ</code></pre><p>where φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀ Bₖ s  is a quadratic approximation of f about xₖ, ψ(s; xₖ) = h(xₖ + s), ‖⋅‖ is a user-defined norm and Δₖ &gt; 0 is the trust-region radius.</p><p>For advanced usage, first define a solver &quot;TRSolver&quot; to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = TRSolver(reg_nlp; χ =  NormLinf(1), subsolver = R2Solver, m_monotone = 1)
solve!(solver, reg_nlp)

stats = RegularizedExecutionStats(reg_nlp)
solve!(solver, reg_nlp, stats)</code></pre><p><strong>Arguments</strong></p><ul><li><code>reg_nlp::AbstractRegularizedNLPModel{T, V}</code>: the problem to solve, see <code>RegularizedProblems.jl</code>, <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess;</li><li><code>atol::T = √eps(T)</code>: absolute tolerance;</li><li><code>rtol::T = √eps(T)</code>: relative tolerance;</li><li><code>neg_tol::T = eps(T)^(1 / 4)</code>: negative tolerance (see stopping conditions below);</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function (negative number means unlimited);</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds;</li><li><code>max_iter::Int = 10000</code>: maximum number of iterations;</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration;</li><li><code>Δk::T = T(1)</code>: initial value of the trust-region radius;</li><li><code>η1::T = √√eps(T)</code>: successful iteration threshold;</li><li><code>η2::T = T(0.9)</code>: very successful iteration threshold;</li><li><code>γ::T = T(3)</code>: trust-region radius parameter multiplier. Must satisfy <code>γ &gt; 1</code>. The trust-region radius is updated as Δ := Δ*γ when the iteration is very successful and Δ := Δ/γ when the iteration is unsuccessful;</li><li><code>m_monotone::Int = 1</code>: monotonicity parameter. By default, TR is monotone but the non-monotone variant will be used if <code>m_monotone &gt; 1</code>;</li><li><code>opnorm_maxiter::Int = 5</code>: how many iterations of the power method to use to compute the operator norm of Bₖ. If a negative number is provided, then Arpack is used instead;</li><li><code>χ::F =  NormLinf(1)</code>: norm used to define the trust-region;`</li><li><code>subsolver::S = R2Solver</code>: subsolver used to solve the subproblem that appears at each iteration.</li><li><code>sub_kwargs::NamedTuple = NamedTuple()</code>: a named tuple containing the keyword arguments to be sent to the subsolver. The solver will fail if invalid keyword arguments are provided to the subsolver. For example, if the subsolver is <code>R2Solver</code>, you can pass <code>sub_kwargs = (max_iter = 100, σmin = 1e-6,)</code>.</li></ul><p>The algorithm stops either when <code>√(ξₖ/νₖ) &lt; atol + rtol*√(ξ₀/ν₀)</code> or <code>ξₖ &lt; 0</code> and <code>√(-ξₖ/νₖ) &lt; neg_tol</code> where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.xk</code>: current iterate;</li><li><code>solver.∇fk</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.solver_specific[:smooth_obj]</code>: current value of the smooth part of the objective function;</li><li><code>stats.solver_specific[:nonsmooth_obj]</code>: current value of the nonsmooth part of the objective function;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything other than <code>:unknown</code> will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention;</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/TR_alg.jl#L100-L155">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.TRDH-Union{Tuple{V}, Tuple{T}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{T, V}, Any, Any, ROSolverOptions{T}}} where {T, V}" href="#RegularizedOptimization.TRDH-Union{Tuple{V}, Tuple{T}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{T, V}, Any, Any, ROSolverOptions{T}}} where {T, V}"><code>RegularizedOptimization.TRDH</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TRDH(reg_nlp; kwargs…)
TRDH(nlp, h, χ, options; kwargs...)
TRDH(f, ∇f!, h, options, x0)</code></pre><p>A trust-region method with diagonal Hessian approximation for the problem</p><pre><code class="nohighlight hljs">min f(x) + h(x)</code></pre><p>where f: ℝⁿ → ℝ has a Lipschitz-continuous gradient,, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.</p><p>About each iterate xₖ, a step sₖ is computed as an approximate solution of</p><pre><code class="nohighlight hljs">min  φ(s; xₖ) + ψ(s; xₖ)  subject to  ‖s‖ ≤ Δₖ</code></pre><p>where φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀ Dₖ s  is a quadratic approximation of f about xₖ, ψ(s; xₖ) = h(xₖ + s), ‖⋅‖ is a user-defined norm, Dₖ is a diagonal Hessian approximation and Δₖ &gt; 0 is the trust-region radius.</p><p>For advanced usage, first define a solver &quot;TRDHSolver&quot; to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = TRDH(reg_nlp; D = nothing, χ =  NormLinf(1))
solve!(solver, reg_nlp)

stats = RegularizedExecutionStats(reg_nlp)
solve!(solver, reg_nlp, stats)</code></pre><p><strong>Arguments</strong></p><ul><li><code>reg_nlp::AbstractRegularizedNLPModel{T, V}</code>: the problem to solve, see <code>RegularizedProblems.jl</code>, <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess;</li><li><code>atol::T = √eps(T)</code>: absolute tolerance;</li><li><code>rtol::T = √eps(T)</code>: relative tolerance;</li><li><code>neg_tol::T = eps(T)^(1 / 4)</code>: negative tolerance;</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function (negative number means unlimited);</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds;</li><li><code>max_iter::Int = 10000</code>: maximum number of iterations;</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration;</li><li><code>Δk::T = T(1)</code>: initial value of the trust-region radius;</li><li><code>η1::T = √√eps(T)</code>: successful iteration threshold;</li><li><code>η2::T = T(0.9)</code>: very successful iteration threshold;</li><li><code>γ::T = T(3)</code>: trust-region radius parameter multiplier. Must satisfy <code>γ &gt; 1</code>. The trust-region radius is updated as Δ := Δ*γ when the iteration is very successful and Δ := Δ/γ when the iteration is unsuccessful;</li><li><code>reduce_TR::Bool = true</code>: see explanation on the stopping criterion below;</li><li><code>χ::F =  NormLinf(1)</code>: norm used to define the trust-region;`</li><li><code>D::L = nothing</code>: diagonal quasi-Newton approximation used for the model φ. If nothing is provided and <code>reg_nlp.model</code> is not a diagonal quasi-Newton approximation, a spectral gradient approximation is used.`</li></ul><p>The algorithm stops either when <code>√(ξₖ/νₖ) &lt; atol + rtol*√(ξ₀/ν₀)</code> or <code>ξₖ &lt; 0</code> and <code>√(-ξₖ/νₖ) &lt; neg_tol</code> where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure. Alternatively, if <code>reduce_TR = true</code>, then ξₖ₁ := f(xₖ) + h(xₖ) - φ(sₖ₁; xₖ) - ψ(sₖ₁; xₖ) is used instead of ξₖ, where sₖ₁ is the Cauchy point.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.xk</code>: current iterate;</li><li><code>solver.∇fk</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.solver_specific[:smooth_obj]</code>: current value of the smooth part of the objective function;</li><li><code>stats.solver_specific[:nonsmooth_obj]</code>: current value of the nonsmooth part of the objective function;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything other than <code>:unknown</code> will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention;</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/TRDH_alg.jl#L93-L149">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RegularizedOptimization.project_y!-Tuple{Percival.AugLagModel}" href="#RegularizedOptimization.project_y!-Tuple{Percival.AugLagModel}"><code>RegularizedOptimization.project_y!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">project_y!(nlp)</code></pre><p>Given an <code>AugLagModel</code>, project <code>nlp.y</code> into [ymin, ymax] and updates <code>nlp.μc_y</code> accordingly.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl/blob/f0b567abae401543003fdd76b2f6811e4b650961/src/AL_alg.jl#L384-L388">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/custom_regularizer/">« Custom regularizers</a><a class="docs-footer-nextpage" href="../bibliography/">Bibliography »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 30 September 2025 06:16">Tuesday 30 September 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
