var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [RegularizedOptimization]","category":"page"},{"location":"reference/#RegularizedOptimization.ALSolver","page":"Reference","title":"RegularizedOptimization.ALSolver","text":"AL(reg_nlp; kwargs...)\n\nAn augmented Lagrangian method for constrained regularized optimization, namely problems in the form\n\nminimize    f(x) + h(x)\nsubject to  lvar ≤ x ≤ uvar,\n            lcon ≤ c(x) ≤ ucon\n\nwhere f: ℝⁿ → ℝ, c: ℝⁿ → ℝᵐ and their derivatives are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAt each iteration, an iterate x is computed as an approximate solution of the subproblem\n\nminimize    L(x;y,μ) + h(x)\nsubject to  lvar ≤ x ≤ uvar\n\nwhere y is an estimate of the Lagrange multiplier vector for the constraints lcon ≤ c(x) ≤ ucon,  μ is the penalty parameter and L(⋅;y,μ) is the augmented Lagrangian function defined by\n\nL(x;y,μ) := f(x) - yᵀc(x) + ½ μ ‖c(x)‖².\n\nFor advanced usage, first define a solver \"ALSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = ALSolver(reg_nlp)\nsolve!(solver, reg_nlp)\n\nstats = GenericExecutionStats(reg_nlp.model)\nsolver = ALSolver(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel: a regularized optimization problem, see RegularizedProblems.jl,  consisting of model representing a smooth optimization problem, see NLPModels.jl, and a regularizer h such as those defined in ProximalOperators.jl.\n\nThe objective and gradient of model will be accessed. The Hessian of model may be accessed or not, depending on the subsolver adopted. If adopted, the Hessian is accessed as an abstract operator and need not be the exact Hessian.\n\nKeyword arguments\n\nx::AbstractVector: a primal initial guess (default: reg_nlp.model.meta.x0)\ny::AbstractVector: a dual initial guess (default: reg_nlp.model.meta.y0)\natol::T = √eps(T): absolute optimality tolerance;\nctol::T = atol: absolute feasibility tolerance;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nmax_iter::Int = 10000: maximum number of iterations;\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nsubsolver::AbstractOptimizationSolver = has_bounds(nlp) ? TR : R2: the procedure used to compute a step (e.g. PG, R2, TR or TRDH);\nsubsolver_logger::AbstractLogger: a logger to pass to the subproblem solver;\ninit_penalty::T = T(10): initial penalty parameter;\nfactor_penalty_up::T = T(2): multiplicative factor to increase the penalty parameter;\nfactor_primal_linear_improvement::T = T(3/4): fraction to declare sufficient improvement of feasibility;\ninit_subtol::T = T(0.1): initial subproblem tolerance;\nfactor_decrease_subtol::T = T(1/4): multiplicative factor to decrease the subproblem tolerance;\ndual_safeguard = (nlp::AugLagModel) -> nothing: in-place function to modify, as needed, the dual estimate.\n\nOutput\n\nstats::GenericExecutionStats: solution and other info, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in reg_nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.y: current dual estimate;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RegularizedOptimization.R2NModel","page":"Reference","title":"RegularizedOptimization.R2NModel","text":"R2NModel(B, ∇f, v, σ, x0)\n\nGiven the unconstrained optimization problem:\n\nmin f(x)\n\nthis model represents the smooth R2N subproblem:\n\nmin_s  f^T s + tfrac12 s^T B s + tfrac12 σ s^2\n\nwhere B is either an approximation of the Hessian of f or the Hessian itself and ∇f represents the gradient of f at x0. σ > 0 is a regularization parameter and v is a vector of the same size as x0 used for intermediary computations.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RegularizedOptimization.AL-Tuple{Val{:equ}, RegularizedProblems.AbstractRegularizedNLPModel}","page":"Reference","title":"RegularizedOptimization.AL","text":"AL(reg_nlp; kwargs...)\n\nAn augmented Lagrangian method for constrained regularized optimization, namely problems in the form\n\nminimize    f(x) + h(x)\nsubject to  lvar ≤ x ≤ uvar,\n            lcon ≤ c(x) ≤ ucon\n\nwhere f: ℝⁿ → ℝ, c: ℝⁿ → ℝᵐ and their derivatives are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAt each iteration, an iterate x is computed as an approximate solution of the subproblem\n\nminimize    L(x;y,μ) + h(x)\nsubject to  lvar ≤ x ≤ uvar\n\nwhere y is an estimate of the Lagrange multiplier vector for the constraints lcon ≤ c(x) ≤ ucon,  μ is the penalty parameter and L(⋅;y,μ) is the augmented Lagrangian function defined by\n\nL(x;y,μ) := f(x) - yᵀc(x) + ½ μ ‖c(x)‖².\n\nFor advanced usage, first define a solver \"ALSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = ALSolver(reg_nlp)\nsolve!(solver, reg_nlp)\n\nstats = GenericExecutionStats(reg_nlp.model)\nsolver = ALSolver(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel: a regularized optimization problem, see RegularizedProblems.jl,  consisting of model representing a smooth optimization problem, see NLPModels.jl, and a regularizer h such as those defined in ProximalOperators.jl.\n\nThe objective and gradient of model will be accessed. The Hessian of model may be accessed or not, depending on the subsolver adopted. If adopted, the Hessian is accessed as an abstract operator and need not be the exact Hessian.\n\nKeyword arguments\n\nx::AbstractVector: a primal initial guess (default: reg_nlp.model.meta.x0)\ny::AbstractVector: a dual initial guess (default: reg_nlp.model.meta.y0)\natol::T = √eps(T): absolute optimality tolerance;\nctol::T = atol: absolute feasibility tolerance;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nmax_iter::Int = 10000: maximum number of iterations;\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nsubsolver::AbstractOptimizationSolver = has_bounds(nlp) ? TR : R2: the procedure used to compute a step (e.g. PG, R2, TR or TRDH);\nsubsolver_logger::AbstractLogger: a logger to pass to the subproblem solver;\ninit_penalty::T = T(10): initial penalty parameter;\nfactor_penalty_up::T = T(2): multiplicative factor to increase the penalty parameter;\nfactor_primal_linear_improvement::T = T(3/4): fraction to declare sufficient improvement of feasibility;\ninit_subtol::T = T(0.1): initial subproblem tolerance;\nfactor_decrease_subtol::T = T(1/4): multiplicative factor to decrease the subproblem tolerance;\ndual_safeguard = (nlp::AugLagModel) -> nothing: in-place function to modify, as needed, the dual estimate.\n\nOutput\n\nstats::GenericExecutionStats: solution and other info, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in reg_nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.y: current dual estimate;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.FISTA-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}","page":"Reference","title":"RegularizedOptimization.FISTA","text":"FISTA for   min_x ϕ(x) = f(x) + g(x), with f(x) cvx and β-smooth, g(x) closed cvx\n\nInput:     f: function handle that returns f(x) and ∇f(x)     h: function handle that returns g(x)     s: initial point     proxG: function handle that calculates prox_{νg}     options: see descentopts.jl   Output:     s⁺: s update     s : s^(k-1)     his : function history     feval : number of function evals (total objective)\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.LM-Union{Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, ROSolverOptions}} where H","page":"Reference","title":"RegularizedOptimization.LM","text":"LM(nls, h, options; kwargs...)\n\nA Levenberg-Marquardt method for the problem\n\nmin ½ ‖F(x)‖² + h(x)\n\nwhere F: ℝⁿ → ℝᵐ and its Jacobian J are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAt each iteration, a step s is computed as an approximate solution of\n\nmin  ½ ‖J(x) s + F(x)‖² + ½ σ ‖s‖² + ψ(s; x)\n\nwhere F(x) and J(x) are the residual and its Jacobian at x, respectively, ψ(s; x) = h(x + s), and σ > 0 is a regularization parameter.\n\nArguments\n\nnls::AbstractNLSModel: a smooth nonlinear least-squares problem\nh: a regularizer such as those defined in ProximalOperators\noptions::ROSolverOptions: a structure containing algorithmic parameters\n\nKeyword arguments\n\nx0::AbstractVector: an initial guess (default: nls.meta.x0)\nsubsolver_logger::AbstractLogger: a logger to pass to the subproblem solver\nsubsolver: the procedure used to compute a step (PG, R2 or TRDH)\nsubsolver_options::ROSolverOptions: default options to pass to the subsolver.\nselected::AbstractVector{<:Integer}: (default 1:nls.meta.nvar).\n\nReturn values\n\nxk: the final iterate\nFobj_hist: an array with the history of values of the smooth objective\nHobj_hist: an array with the history of values of the nonsmooth objective\nComplex_hist: an array with the history of number of inner iterations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.LMTR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, X, ROSolverOptions}} where {H, X}","page":"Reference","title":"RegularizedOptimization.LMTR","text":"LMTR(nls, h, χ, options; kwargs...)\n\nA trust-region Levenberg-Marquardt method for the problem\n\nmin ½ ‖F(x)‖² + h(x)\n\nwhere F: ℝⁿ → ℝᵐ and its Jacobian J are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous and proper.\n\nAt each iteration, a step s is computed as an approximate solution of\n\nmin  ½ ‖J(x) s + F(x)‖₂² + ψ(s; x)  subject to  ‖s‖ ≤ Δ\n\nwhere F(x) and J(x) are the residual and its Jacobian at x, respectively, ψ(s; x) = h(x + s), ‖⋅‖ is a user-defined norm and Δ > 0 is a trust-region radius.\n\nArguments\n\nnls::AbstractNLSModel: a smooth nonlinear least-squares problem\nh: a regularizer such as those defined in ProximalOperators\nχ: a norm used to define the trust region in the form of a regularizer\noptions::ROSolverOptions: a structure containing algorithmic parameters\n\nKeyword arguments\n\nx0::AbstractVector: an initial guess (default: nls.meta.x0)\nsubsolver_logger::AbstractLogger: a logger to pass to the subproblem solver\nsubsolver: the procedure used to compute a step (PG, R2 or TRDH)\nsubsolver_options::ROSolverOptions: default options to pass to the subsolver.\nselected::AbstractVector{<:Integer}: (default 1:nls.meta.nvar).\n\nReturn values\n\nxk: the final iterate\nFobj_hist: an array with the history of values of the smooth objective\nHobj_hist: an array with the history of values of the nonsmooth objective\nComplex_hist: an array with the history of number of inner iterations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.PG-Tuple{NLPModels.AbstractNLPModel, Vararg{Any}}","page":"Reference","title":"RegularizedOptimization.PG","text":"Proximal Gradient Descent  for\n\nmin_x ϕ(x) = f(x) + g(x), with f(x) β-smooth, g(x) closed, lsc\n\nInput:   f: function handle that returns f(x) and ∇f(x)   h: function handle that returns g(x)   s: initial point   proxG: function handle that calculates prox_{νg}   options: see descentopts.jl Output:   s⁺: s update   s : s^(k-1)   his : function history   feval : number of function evals (total objective )\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.R2-Union{Tuple{V}, Tuple{R}, Tuple{NLPModels.AbstractNLPModel{R, V}, Any, ROSolverOptions{R}}} where {R<:Real, V}","page":"Reference","title":"RegularizedOptimization.R2","text":"R2(reg_nlp; kwargs…)\n\nA first-order quadratic regularization method for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ has a Lipschitz-continuous gradient, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAbout each iterate xₖ, a step sₖ is computed as a solution of\n\nmin  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs is the Taylor linear approximation of f about xₖ, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is a user-defined norm and σₖ > 0 is the regularization parameter.\n\nFor advanced usage, first define a solver \"R2Solver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = R2Solver(reg_nlp)\nsolve!(solver, reg_nlp)\n\nstats = RegularizedExecutionStats(reg_nlp)\nsolver = R2Solver(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\natol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nneg_tol::T = eps(T)^(1 / 4): negative tolerance\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nσmin::T = eps(T): minimum value of the regularization parameter;\nη1::T = √√eps(T): very successful iteration threshold;\nη2::T = T(0.9): successful iteration threshold;\nν::T = eps(T)^(1 / 5): multiplicative inverse of the regularization parameter: ν = 1/σ;\nγ::T = T(3): regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful.\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.R2DH-Union{Tuple{V}, Tuple{T}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{T, V}, Any, ROSolverOptions{T}}} where {T, V}","page":"Reference","title":"RegularizedOptimization.R2DH","text":"R2DH(reg_nlp; kwargs…)\n\nA second-order quadratic regularization method for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ is C¹, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAbout each iterate xₖ, a step sₖ is computed as a solution of\n\nmin  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀDₖs is a diagonal quadratic approximation of f about xₖ, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is the ℓ₂ norm and σₖ > 0 is the regularization parameter.\n\nFor advanced usage, first define a solver R2DHSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = R2DHSolver(reg_nlp; m_monotone = 6)\nsolve!(solver, reg_nlp)\n\nor\n\nstats = RegularizedExecutionStats(reg_nlp)\nsolver = R2DHSolver(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\natol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nneg_tol::T = eps(T)^(1 / 4): negative tolerance\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nσmin::T = eps(T): minimum value of the regularization parameter;\nσk::T = eps(T)^(1 / 5): initial value of the regularization parameter;\nη1::T = √√eps(T): very successful iteration threshold;\nη2::T = T(0.9): successful iteration threshold;\nγ::T = T(3): regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful.\nθ::T = 1/(1 + eps(T)^(1 / 5)): is the model decrease fraction with respect to the decrease of the Cauchy model. \nm_monotone::Int = 6: monotoneness parameter. By default, R2DH is non-monotone but the monotone variant can be used with m_monotone = 1\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.R2N-Union{Tuple{V}, Tuple{T}, Tuple{NLPModels.AbstractNLPModel{T, V}, Any, ROSolverOptions{T}}} where {T<:Real, V}","page":"Reference","title":"RegularizedOptimization.R2N","text":"R2N(reg_nlp; kwargs…)\n\nA second-order quadratic regularization method for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ is C¹, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAbout each iterate xₖ, a step sₖ is computed as a solution of\n\nmin  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀBₖs is a quadratic approximation of f about xₖ, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is the ℓ₂ norm and σₖ > 0 is the regularization parameter.\n\nFor advanced usage, first define a solver \"R2NSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = R2NSolver(reg_nlp; m_monotone = 1)\nsolve!(solver, reg_nlp)\n\nstats = RegularizedExecutionStats(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\natol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nneg_tol::T = eps(T)^(1 / 4): negative tolerance;\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nσmin::T = eps(T): minimum value of the regularization parameter;\nσk::T = eps(T)^(1 / 5): initial value of the regularization parameter;\nη1::T = √√eps(T): successful iteration threshold;\nη2::T = T(0.9): very successful iteration threshold;\nγ::T = T(3): regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful;\nθ::T = 1/(1 + eps(T)^(1 / 5)): is the model decrease fraction with respect to the decrease of the Cauchy model;\nm_monotone::Int = 1: monotonicity parameter. By default, R2N is monotone but the non-monotone variant will be used if m_monotone > 1;\nsub_kwargs::Dict{Symbol}: a dictionary containing the keyword arguments to be sent to the subsolver. The solver will fail if invalid keyword arguments are provided to the subsolver.\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything other than :unknown will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.RegularizedExecutionStats-Union{Tuple{RegularizedProblems.AbstractRegularizedNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}","page":"Reference","title":"RegularizedOptimization.RegularizedExecutionStats","text":"GenericExecutionStats(reg_nlp :: AbstractRegularizedNLPModel{T, V})\n\nConstruct a GenericExecutionStats object from an AbstractRegularizedNLPModel.  More specifically, construct a GenericExecutionStats on the NLPModel of regnlp and add three solverspecific entries namely :smoothobj, :nonsmoothobj and :xi. This is useful for reducing the number of allocations when calling solve!(..., regnlp, stats) and should be used by default. Warning: This should not be used when adding other solverspecific entries that do not have the current scalar type. \n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.TR-Union{Tuple{R}, Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLPModel, H, X, ROSolverOptions{R}}} where {H, X, R}","page":"Reference","title":"RegularizedOptimization.TR","text":"TR(nlp, h, χ, options; kwargs...)\n\nA trust-region method for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ has a Lipschitz-continuous Jacobian, and h: ℝⁿ → ℝ is lower semi-continuous and proper.\n\nAbout each iterate xₖ, a step sₖ is computed as an approximate solution of\n\nmin  φ(s; xₖ) + ψ(s; xₖ)  subject to  ‖s‖ ≤ Δₖ\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀ Bₖ s  is a quadratic approximation of f about xₖ, ψ(s; xₖ) = h(xₖ + s), ‖⋅‖ is a user-defined norm and Δₖ > 0 is the trust-region radius. The subproblem is solved inexactly by way of a first-order method such as the proximal-gradient method or the quadratic regularization method.\n\nArguments\n\nnlp::AbstractNLPModel: a smooth optimization problem\nh: a regularizer such as those defined in ProximalOperators\nχ: a norm used to define the trust region in the form of a regularizer\noptions::ROSolverOptions: a structure containing algorithmic parameters\n\nThe objective, gradient and Hessian of nlp will be accessed. The Hessian is accessed as an abstract operator and need not be the exact Hessian.\n\nKeyword arguments\n\nx0::AbstractVector: an initial guess (default: nlp.meta.x0)\nsubsolver_logger::AbstractLogger: a logger to pass to the subproblem solver (default: the null logger)\nsubsolver: the procedure used to compute a step (PG, R2 or TRDH)\nsubsolver_options::ROSolverOptions: default options to pass to the subsolver (default: all defaut options)\nselected::AbstractVector{<:Integer}: (default 1:f.meta.nvar).\n\nReturn values\n\nxk: the final iterate\nFobj_hist: an array with the history of values of the smooth objective\nHobj_hist: an array with the history of values of the nonsmooth objective\nComplex_hist: an array with the history of number of inner iterations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.TRDH-Union{Tuple{S}, Tuple{R}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{R, S}, Any, Any, ROSolverOptions{R}}} where {R<:Real, S}","page":"Reference","title":"RegularizedOptimization.TRDH","text":"TRDH(nlp, h, χ, options; kwargs...)\nTRDH(f, ∇f!, h, options, x0)\n\nA trust-region method with diagonal Hessian approximation for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ has a Lipschitz-continuous Jacobian, and h: ℝⁿ → ℝ is lower semi-continuous and proper.\n\nAbout each iterate xₖ, a step sₖ is computed as an approximate solution of\n\nmin  φ(s; xₖ) + ψ(s; xₖ)  subject to  ‖s‖ ≤ Δₖ\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀ Dₖ s  is a quadratic approximation of f about xₖ, ψ(s; xₖ) = h(xₖ + s), ‖⋅‖ is a user-defined norm, Dₖ is a diagonal Hessian approximation and Δₖ > 0 is the trust-region radius.\n\nArguments\n\nnlp::AbstractDiagonalQNModel: a smooth optimization problem\nh: a regularizer such as those defined in ProximalOperators\nχ: a norm used to define the trust region in the form of a regularizer\noptions::ROSolverOptions: a structure containing algorithmic parameters\n\nThe objective and gradient of nlp will be accessed.\n\nIn the second form, instead of nlp, the user may pass in\n\nf a function such that f(x) returns the value of f at x\n∇f! a function to evaluate the gradient in place, i.e., such that ∇f!(g, x) store ∇f(x) in g\nx0::AbstractVector: an initial guess.\n\nKeyword arguments\n\nx0::AbstractVector: an initial guess (default: nlp.meta.x0)\nselected::AbstractVector{<:Integer}: (default 1:f.meta.nvar)\n\nReturn values\n\nxk: the final iterate\nFobj_hist: an array with the history of values of the smooth objective\nHobj_hist: an array with the history of values of the nonsmooth objective\nComplex_hist: an array with the history of number of inner iterations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.project_y!-Tuple{Percival.AugLagModel}","page":"Reference","title":"RegularizedOptimization.project_y!","text":"project_y!(nlp)\n\nGiven an AugLagModel, project nlp.y into [ymin, ymax] and updates nlp.μc_y accordingly.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.prox_split_1w-NTuple{4, Any}","page":"Reference","title":"RegularizedOptimization.prox_split_1w","text":"Solves descent direction s for some objective function with the structure \tmins qk(s) + ψ(x+s) s.t. ||s||q⩽ Δ \tfor some Δ provided Arguments ––––– proxp : prox method for p-norm \ttakes in z (vector), a (λ||⋅||p), p is norm for ψ I think s0 : Vector{Float64,1} \tInitial guess for the descent direction projq : generic that projects onto ||⋅||q⩽Δ norm ball options : mutable structure pparams\n\nReturns\n\ns   : Vector{Float64,1} \tFinal value of Algorithm 6.1 descent direction w   : Vector{Float64,1} \trelaxation variable of Algorithm 6.1 descent direction\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.prox_split_2w-NTuple{4, Any}","page":"Reference","title":"RegularizedOptimization.prox_split_2w","text":"Solves descent direction s for some objective function with the structure \tmins qk(s) + ψ(x+s) s.t. ||s||q⩽ Δ \tfor some Δ provided Arguments ––––– proxp : prox method for p-norm \ttakes in z (vector), a (λ||⋅||p), p is norm for ψ I think s0 : Vector{Float64,1} \tInitial guess for the descent direction projq : generic that projects onto ||⋅||q⩽Δ norm ball options : mutable structure pparams\n\nReturns\n\ns   : Vector{Float64,1} \tFinal value of Algorithm 6.2 descent direction w   : Vector{Float64,1} \trelaxation variable of Algorithm 6.2 descent direction\n\n\n\n\n\n","category":"method"},{"location":"#RegularizedOptimization.jl","page":"Home","title":"RegularizedOptimization.jl","text":"","category":"section"},{"location":"tutorial/#RegularizedOptimization-Tutorial","page":"Tutorial","title":"RegularizedOptimization Tutorial","text":"","category":"section"}]
}
