var documenterSearchIndex = {"docs":
[{"location":"examples/basic/#A-regularized-optimization-problem","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"","category":"section"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"In this tutorial, we will show how to model and solve the nonconvex nonsmooth optimization problem","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"  min_x in mathbbR^2 (1 - x_1)^2 + 100(x_2 - x_1^2)^2 + x_1 + x_2","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"which can be seen as a ell_1 regularization of the Rosenbrock function.  It can be shown that the solution to the problem is ","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"  x^* = beginpmatrix\n  025\n  00575\n  endpmatrix","category":"page"},{"location":"examples/basic/#Modelling-the-problem","page":"A regularized optimization problem","title":"Modelling the problem","text":"","category":"section"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"We first formulate the objective function as the sum of a smooth function f and a nonsmooth regularizer h:","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"  (1 - x_1)^2 + 100(x_2 - x_1^2)^2 + x_1 + x_2 = f(x_1 x_2) + h(x_1 x_2)","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"where ","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"beginalign*\nf(x_1 x_2) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2\nh(x_1 x_2) = x_1\nendalign*","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"To model f, we are going to use ADNLPModels.jl. For the nonsmooth regularizer, we use ProximalOperators.jl.  We then wrap the smooth function and the regularizer in a RegularizedNLPModel","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"using ADNLPModels\nusing ProximalOperators\nusing RegularizedProblems\n\n# Model the function\nf_fun = x -> (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2\n\n# Choose a starting point for the optimization process, for the sake of this example, we choose\nx0 = [-1.0, 2.0]\n\n# Get an NLPModel corresponding to the smooth function f\nf_model = ADNLPModel(f_fun, x0, name = \"AD model of f\") \n\n# Get the regularizer from ProximalOperators\nh = NormL1(1.0)\n\n# Wrap into a RegularizedNLPModel\nregularized_pb = RegularizedNLPModel(f_model, h)","category":"page"},{"location":"examples/basic/#Solving-the-problem","page":"A regularized optimization problem","title":"Solving the problem","text":"","category":"section"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"We can now choose one of the solvers presented here to solve the problem we defined above. Please refer to other sections of this documentation to make the wisest choice for your particular problem. Depending on the problem structure and on requirements from the user, some solvers are more appropriate than others. The following tries to give a quick overview of what choices one can make.","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"Suppose for example that we don't want to use a quasi-Newton approach and that we don't have access to the Hessian of f, or that we don't want to incur the cost of computing it.  In this case, the most appropriate solver would be R2. For this example, we also choose a tolerance by specifying the keyword arguments atol and rtol across all solvers.","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"using RegularizedOptimization\n \nout = R2(regularized_pb, verbose = 100, atol = 1e-6, rtol = 1e-6)\nprintln(\"R2 converged after $(out.iter) iterations to the solution x = $(out.solution)\")","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"Now, we can actually use second information on f.  To do so, we are going to use TR, a trust-region solver that can exploit second order information.","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"\nout = TR(regularized_pb, verbose = 100, atol = 1e-6, rtol = 1e-6)\nprintln(\"TR converged after $(out.iter) iterations to the solution x = $(out.solution)\")","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"Suppose for some reason we can not compute the Hessian.  In this case, we can try to switch to a quasi-Newton approximation, this can be done with NLPModelsModifiers.jl We could choose to use TR again but for the sake of this tutorial we run it with R2N","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"using NLPModelsModifiers\n\n# Switch the model of the smooth function to a quasi-Newton approximation\nf_model_lsr1 = LSR1Model(f_model)\nregularized_pb_lsr1 = RegularizedNLPModel(f_model_lsr1, h)\n\n# Solve with R2N\nout = R2N(regularized_pb_lsr1, verbose = 100, atol = 1e-6, rtol = 1e-6)\nprintln(\"R2N converged after $(out.iter) iterations to the solution x = $(out.solution)\")","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"Finally, TRDH and R2DH are specialized for diagonal quasi-Newton approximations, and should be used instead of TR and R2N, respectively.","category":"page"},{"location":"examples/basic/","page":"A regularized optimization problem","title":"A regularized optimization problem","text":"\nf_model_sg = SpectralGradientModel(f_model)\nregularized_pb_sg = RegularizedNLPModel(f_model_sg, h)\n\n# Solve with R2DH\nout = R2DH(regularized_pb_sg, verbose = 100, atol = 1e-6, rtol = 1e-6)\nprintln(\"R2DH converged after $(out.iter) iterations to the solution x = $(out.solution)\")","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/#RegularizedOptimization.ALSolver","page":"Reference","title":"RegularizedOptimization.ALSolver","text":"AL(reg_nlp; kwargs...)\n\nAn augmented Lagrangian method for constrained regularized optimization, namely problems in the form\n\nminimize    f(x) + h(x)\nsubject to  lvar ≤ x ≤ uvar,\n            lcon ≤ c(x) ≤ ucon\n\nwhere f: ℝⁿ → ℝ, c: ℝⁿ → ℝᵐ and their derivatives are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAt each iteration, an iterate x is computed as an approximate solution of the subproblem\n\nminimize    L(x;y,μ) + h(x)\nsubject to  lvar ≤ x ≤ uvar\n\nwhere y is an estimate of the Lagrange multiplier vector for the constraints lcon ≤ c(x) ≤ ucon,  μ is the penalty parameter and L(⋅;y,μ) is the augmented Lagrangian function defined by\n\nL(x;y,μ) := f(x) - yᵀc(x) + ½ μ ‖c(x)‖².\n\nFor advanced usage, first define a solver \"ALSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = ALSolver(reg_nlp)\nsolve!(solver, reg_nlp)\n\nstats = GenericExecutionStats(reg_nlp.model)\nsolver = ALSolver(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel: a regularized optimization problem, see RegularizedProblems.jl,  consisting of model representing a smooth optimization problem, see NLPModels.jl, and a regularizer h such as those defined in ProximalOperators.jl.\n\nThe objective and gradient of model will be accessed. The Hessian of model may be accessed or not, depending on the subsolver adopted. If adopted, the Hessian is accessed as an abstract operator and need not be the exact Hessian.\n\nKeyword arguments\n\nx::AbstractVector: a primal initial guess (default: reg_nlp.model.meta.x0)\ny::AbstractVector: a dual initial guess (default: reg_nlp.model.meta.y0)\natol::T = √eps(T): absolute optimality tolerance;\nctol::T = atol: absolute feasibility tolerance;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nmax_iter::Int = 10000: maximum number of iterations;\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nsubsolver::AbstractOptimizationSolver = has_bounds(nlp) ? TR : R2: the procedure used to compute a step (e.g. PG, R2, TR or TRDH);\nsubsolver_logger::AbstractLogger: a logger to pass to the subproblem solver;\ninit_penalty::T = T(10): initial penalty parameter;\nfactor_penalty_up::T = T(2): multiplicative factor to increase the penalty parameter;\nfactor_primal_linear_improvement::T = T(3/4): fraction to declare sufficient improvement of feasibility;\ninit_subtol::T = T(0.1): initial subproblem tolerance;\nfactor_decrease_subtol::T = T(1/4): multiplicative factor to decrease the subproblem tolerance;\ndual_safeguard = (nlp::AugLagModel) -> nothing: in-place function to modify, as needed, the dual estimate.\n\nOutput\n\nstats::GenericExecutionStats: solution and other info, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in reg_nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.y: current dual estimate;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RegularizedOptimization.LMModel","page":"Reference","title":"RegularizedOptimization.LMModel","text":"LMModel(j_prod!, jt_prod, F, v, σ, xk)\n\nGiven the unconstrained optimization problem:\n\nmin tfrac12  F(x) ^2\n\nthis model represents the smooth LM subproblem:\n\nmin_s  tfrac12  F(x) + J(x)s ^2 + tfrac12 σ s^2\n\nwhere J is the Jacobian of F at xk, represented via matrix-free operations. j_prod!(xk, s, out) computes J(xk) * s, and jt_prod!(xk, r, out) computes J(xk)' * r.\n\nσ > 0 is a regularization parameter and v is a vector of the same size as F(xk) used for intermediary computations.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RegularizedOptimization.R2NModel","page":"Reference","title":"RegularizedOptimization.R2NModel","text":"R2NModel(B, ∇f, v, σ, x0)\n\nGiven the unconstrained optimization problem:\n\nmin f(x)\n\nthis model represents the smooth R2N subproblem:\n\nmin_s  f^T s + tfrac12 s^T B s + tfrac12 σ s^2\n\nwhere B is either an approximation of the Hessian of f or the Hessian itself and ∇f represents the gradient of f at x0. σ > 0 is a regularization parameter and v is a vector of the same size as x0 used for intermediary computations.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RegularizedOptimization.AL-Tuple{Val{:equ}, RegularizedProblems.AbstractRegularizedNLPModel}","page":"Reference","title":"RegularizedOptimization.AL","text":"AL(reg_nlp; kwargs...)\n\nAn augmented Lagrangian method for constrained regularized optimization, namely problems in the form\n\nminimize    f(x) + h(x)\nsubject to  lvar ≤ x ≤ uvar,\n            lcon ≤ c(x) ≤ ucon\n\nwhere f: ℝⁿ → ℝ, c: ℝⁿ → ℝᵐ and their derivatives are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAt each iteration, an iterate x is computed as an approximate solution of the subproblem\n\nminimize    L(x;y,μ) + h(x)\nsubject to  lvar ≤ x ≤ uvar\n\nwhere y is an estimate of the Lagrange multiplier vector for the constraints lcon ≤ c(x) ≤ ucon,  μ is the penalty parameter and L(⋅;y,μ) is the augmented Lagrangian function defined by\n\nL(x;y,μ) := f(x) - yᵀc(x) + ½ μ ‖c(x)‖².\n\nFor advanced usage, first define a solver \"ALSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = ALSolver(reg_nlp)\nsolve!(solver, reg_nlp)\n\nstats = GenericExecutionStats(reg_nlp.model)\nsolver = ALSolver(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel: a regularized optimization problem, see RegularizedProblems.jl,  consisting of model representing a smooth optimization problem, see NLPModels.jl, and a regularizer h such as those defined in ProximalOperators.jl.\n\nThe objective and gradient of model will be accessed. The Hessian of model may be accessed or not, depending on the subsolver adopted. If adopted, the Hessian is accessed as an abstract operator and need not be the exact Hessian.\n\nKeyword arguments\n\nx::AbstractVector: a primal initial guess (default: reg_nlp.model.meta.x0)\ny::AbstractVector: a dual initial guess (default: reg_nlp.model.meta.y0)\natol::T = √eps(T): absolute optimality tolerance;\nctol::T = atol: absolute feasibility tolerance;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nmax_iter::Int = 10000: maximum number of iterations;\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nsubsolver::AbstractOptimizationSolver = has_bounds(nlp) ? TR : R2: the procedure used to compute a step (e.g. PG, R2, TR or TRDH);\nsubsolver_logger::AbstractLogger: a logger to pass to the subproblem solver;\ninit_penalty::T = T(10): initial penalty parameter;\nfactor_penalty_up::T = T(2): multiplicative factor to increase the penalty parameter;\nfactor_primal_linear_improvement::T = T(3/4): fraction to declare sufficient improvement of feasibility;\ninit_subtol::T = T(0.1): initial subproblem tolerance;\nfactor_decrease_subtol::T = T(1/4): multiplicative factor to decrease the subproblem tolerance;\ndual_safeguard = (nlp::AugLagModel) -> nothing: in-place function to modify, as needed, the dual estimate.\n\nOutput\n\nstats::GenericExecutionStats: solution and other info, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in reg_nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.y: current dual estimate;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.LM-Union{Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, ROSolverOptions}} where H","page":"Reference","title":"RegularizedOptimization.LM","text":"LM(reg_nls; kwargs...)\n\nA Levenberg-Marquardt method for the problem\n\nmin ½ ‖F(x)‖² + h(x)\n\nwhere F: ℝⁿ → ℝᵐ and its Jacobian J are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAt each iteration, a step s is computed as an approximate solution of\n\nmin  ½ ‖J(x) s + F(x)‖² + ½ σ ‖s‖² + ψ(s; x)\n\nwhere F(x) and J(x) are the residual and its Jacobian at x, respectively, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is the ℓ₂ norm and σₖ > 0 is the regularization parameter.\n\nFor advanced usage, first define a solver \"LMSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = LMSolver(reg_nls; subsolver = R2Solver, m_monotone = 1)\nsolve!(solver, reg_nls)\n\nstats = RegularizedExecutionStats(reg_nls)\nsolve!(solver, reg_nls, stats)\n\nArguments\n\nreg_nls::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\nnonlinear::Bool = true: whether the function F is nonlinear or not.\natol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\n`neg_tol::T = zero(T): negative tolerance;\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nσmin::T = eps(T): minimum value of the regularization parameter;\nσk::T = eps(T)^(1 / 5): initial value of the regularization parameter;\nη1::T = √√eps(T): successful iteration threshold;\nη2::T = T(0.9): very successful iteration threshold;\nγ::T = T(3): regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful;\nθ::T = 1/(1 + eps(T)^(1 / 5)): is the model decrease fraction with respect to the decrease of the Cauchy model;\nm_monotone::Int = 1: monotonicity parameter. By default, LM is monotone but the non-monotone variant will be used if m_monotone > 1;\nsubsolver = R2Solver: the solver used to solve the subproblems.\nsub_kwargs::NamedTuple = NamedTuple(): a named tuple containing the keyword arguments to be sent to the subsolver. The solver will fail if invalid keyword arguments are provided to the subsolver. For example, if the subsolver is R2Solver, you can pass sub_kwargs = (max_iter = 100, σmin = 1e-6,).\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything other than :unknown will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.LMTR-Union{Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLSModel, H, X, ROSolverOptions}} where {H, X}","page":"Reference","title":"RegularizedOptimization.LMTR","text":"LMTR(reg_nls; kwargs...)\nLMTR(nls, h, χ, options; kwargs...)\n\nA trust-region Levenberg-Marquardt method for the problem\n\nmin ½ ‖F(x)‖² + h(x)\n\nwhere F: ℝⁿ → ℝᵐ and its Jacobian J are Lipschitz continuous and h: ℝⁿ → ℝ is lower semi-continuous and proper.\n\nAt each iteration, a step s is computed as an approximate solution of\n\nmin  ½ ‖J(x) s + F(x)‖₂² + ψ(s; x)  subject to  ‖s‖ ≤ Δ\n\nwhere F(x) and J(x) are the residual and its Jacobian at x, respectively, ψ(s; x) = h(x + s), ‖⋅‖ is a user-defined norm and Δ > 0 is a trust-region radius.\n\nFor advanced usage, first define a solver \"LMSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = LMTRSolver(reg_nls; χ = NormLinf(one(T)), subsolver = R2Solver)\nsolve!(solver, reg_nls)\n\nstats = RegularizedExecutionStats(reg_nls)\nsolve!(solver, reg_nls, stats)\n\nArguments\n\nreg_nls::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\natol::T = √eps(T): absolute tolerance;\nsub_atol::T = atol: subsolver absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\n`neg_tol::T = zero(T): negative tolerance;\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nΔk::T = eps(T): initial value of the trust-region radius;\nη1::T = √√eps(T): successful iteration threshold;\nη2::T = T(0.9): very successful iteration threshold;\nγ::T = T(3): trust-region radius parameter multiplier, Δ := Δ*γ when the iteration is very successful and Δ := Δ/γ when the iteration is unsuccessful;\nα::T = 1/eps(T): TODO\nβ::T = 1/eps(T): TODO\nχ =  NormLinf(1): norm used to define the trust-region;`\nsubsolver::S = R2Solver: subsolver used to solve the subproblem that appears at each iteration.\nsub_kwargs::NamedTuple = NamedTuple(): a named tuple containing the keyword arguments to be sent to the subsolver. The solver will fail if invalid keyword arguments are provided to the subsolver. For example, if the subsolver is R2Solver, you can pass sub_kwargs = (max_iter = 100, σmin = 1e-6,).\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything other than :unknown will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.R2-Union{Tuple{V}, Tuple{R}, Tuple{NLPModels.AbstractNLPModel{R, V}, Any, ROSolverOptions{R}}} where {R<:Real, V}","page":"Reference","title":"RegularizedOptimization.R2","text":"R2(reg_nlp; kwargs…)\n\nA first-order quadratic regularization method for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ has a Lipschitz-continuous gradient, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAbout each iterate xₖ, a step sₖ is computed as a solution of\n\nmin  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs is the Taylor linear approximation of f about xₖ, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is a user-defined norm and σₖ > 0 is the regularization parameter.\n\nFor advanced usage, first define a solver \"R2Solver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = R2Solver(reg_nlp)\nsolve!(solver, reg_nlp)\n\nstats = RegularizedExecutionStats(reg_nlp)\nsolver = R2Solver(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\natol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nneg_tol::T = eps(T)^(1 / 4): negative tolerance\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nσmin::T = eps(T): minimum value of the regularization parameter;\nη1::T = √√eps(T): very successful iteration threshold;\nη2::T = T(0.9): successful iteration threshold;\nν::T = eps(T)^(1 / 5): multiplicative inverse of the regularization parameter: ν = 1/σ;\nγ::T = T(3): regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful.\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything other than :unknown will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.R2DH-Union{Tuple{V}, Tuple{T}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{T, V}, Any, ROSolverOptions{T}}} where {T, V}","page":"Reference","title":"RegularizedOptimization.R2DH","text":"R2DH(reg_nlp; kwargs…)\n\nA second-order quadratic regularization method for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ is C¹, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAbout each iterate xₖ, a step sₖ is computed as a solution of\n\nmin  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀDₖs is a diagonal quadratic approximation of f about xₖ, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is the ℓ₂ norm and σₖ > 0 is the regularization parameter.\n\nFor advanced usage, first define a solver R2DHSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = R2DHSolver(reg_nlp; m_monotone = 6)\nsolve!(solver, reg_nlp)\n\nor\n\nstats = RegularizedExecutionStats(reg_nlp)\nsolver = R2DHSolver(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\natol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nneg_tol::T = eps(T)^(1 / 4): negative tolerance\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nσmin::T = eps(T): minimum value of the regularization parameter;\nσk::T = eps(T)^(1 / 5): initial value of the regularization parameter;\nη1::T = √√eps(T): very successful iteration threshold;\nη2::T = T(0.9): successful iteration threshold;\nγ::T = T(3): regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful.\nθ::T = 1/(1 + eps(T)^(1 / 5)): is the model decrease fraction with respect to the decrease of the Cauchy model. \nm_monotone::Int = 6: monotoneness parameter. By default, R2DH is non-monotone but the monotone variant can be used with m_monotone = 1\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything other than :unknown will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.R2N-Union{Tuple{V}, Tuple{T}, Tuple{NLPModels.AbstractNLPModel{T, V}, Any, ROSolverOptions{T}}} where {T<:Real, V}","page":"Reference","title":"RegularizedOptimization.R2N","text":"R2N(reg_nlp; kwargs…)\n\nA second-order quadratic regularization method for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ is C¹, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAbout each iterate xₖ, a step sₖ is computed as a solution of\n\nmin  φ(s; xₖ) + ½ σₖ ‖s‖² + ψ(s; xₖ)\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀBₖs is a quadratic approximation of f about xₖ, ψ(s; xₖ) is either h(xₖ + s) or an approximation of h(xₖ + s), ‖⋅‖ is the ℓ₂ norm and σₖ > 0 is the regularization parameter.\n\nFor advanced usage, first define a solver \"R2NSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = R2NSolver(reg_nlp; m_monotone = 1)\nsolve!(solver, reg_nlp)\n\nstats = RegularizedExecutionStats(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\natol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nneg_tol::T = eps(T)^(1 / 4): negative tolerance;\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nσmin::T = eps(T): minimum value of the regularization parameter;\nσk::T = eps(T)^(1 / 5): initial value of the regularization parameter;\nη1::T = √√eps(T): successful iteration threshold;\nη2::T = T(0.9): very successful iteration threshold;\nγ::T = T(3): regularization parameter multiplier, σ := σ/γ when the iteration is very successful and σ := σγ when the iteration is unsuccessful;\nθ::T = 1/(1 + eps(T)^(1 / 5)): is the model decrease fraction with respect to the decrease of the Cauchy model;\nopnorm_maxiter::Int = 5: how many iterations of the power method to use to compute the operator norm of Bₖ. If a negative number is provided, then Arpack is used instead;\nm_monotone::Int = 1: monotonicity parameter. By default, R2N is monotone but the non-monotone variant will be used if m_monotone > 1;\nsub_kwargs::NamedTuple = NamedTuple(): a named tuple containing the keyword arguments to be sent to the subsolver. The solver will fail if invalid keyword arguments are provided to the subsolver. For example, if the subsolver is R2Solver, you can pass sub_kwargs = (max_iter = 100, σmin = 1e-6,).\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything other than :unknown will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds.\n\nSimilarly to the callback, when using a quasi-Newton approximation, two functions, qn_update_y!(nlp, solver, stats) and qn_copy!(nlp, solver, stats) are called at each update of the approximation. Namely, the former computes the y vector for which the pair (s, y) is pushed into the approximation. By default, y := ∇fk⁻ - ∇fk. The latter allows the user to tell which values should be copied for the next iteration. By default, only the gradient is copied: ∇fk⁻ .= ∇fk. This might be useful when using R2N in a constrained optimization context, when the gradient of the Lagrangian function is pushed at each iteration rather than the gradient of the objective function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.RegularizedExecutionStats-Union{Tuple{RegularizedProblems.AbstractRegularizedNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}","page":"Reference","title":"RegularizedOptimization.RegularizedExecutionStats","text":"GenericExecutionStats(reg_nlp :: AbstractRegularizedNLPModel{T, V})\n\nConstruct a GenericExecutionStats object from an AbstractRegularizedNLPModel.  More specifically, construct a GenericExecutionStats on the NLPModel of regnlp and add three solverspecific entries namely :smoothobj, :nonsmoothobj and :xi. This is useful for reducing the number of allocations when calling solve!(..., regnlp, stats) and should be used by default. Warning: This should not be used when adding other solverspecific entries that do not have the current scalar type. \n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.TR-Union{Tuple{R}, Tuple{X}, Tuple{H}, Tuple{NLPModels.AbstractNLPModel, H, X, ROSolverOptions{R}}} where {H, X, R}","page":"Reference","title":"RegularizedOptimization.TR","text":"TR(reg_nlp; kwargs…)\nTR(nlp, h, χ, options; kwargs...)\n\nA trust-region method for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ has a Lipschitz-continuous gradient, and h: ℝⁿ → ℝ is lower semi-continuous and proper.\n\nAbout each iterate xₖ, a step sₖ is computed as an approximate solution of\n\nmin  φ(s; xₖ) + ψ(s; xₖ)  subject to  ‖s‖ ≤ Δₖ\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀ Bₖ s  is a quadratic approximation of f about xₖ, ψ(s; xₖ) = h(xₖ + s), ‖⋅‖ is a user-defined norm and Δₖ > 0 is the trust-region radius.\n\nFor advanced usage, first define a solver \"TRSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TRSolver(reg_nlp; χ =  NormLinf(1), subsolver = R2Solver, m_monotone = 1)\nsolve!(solver, reg_nlp)\n\nstats = RegularizedExecutionStats(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\natol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nneg_tol::T = eps(T)^(1 / 4): negative tolerance (see stopping conditions below);\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nΔk::T = T(1): initial value of the trust-region radius;\nη1::T = √√eps(T): successful iteration threshold;\nη2::T = T(0.9): very successful iteration threshold;\nγ::T = T(3): trust-region radius parameter multiplier. Must satisfy γ > 1. The trust-region radius is updated as Δ := Δ*γ when the iteration is very successful and Δ := Δ/γ when the iteration is unsuccessful;\nm_monotone::Int = 1: monotonicity parameter. By default, TR is monotone but the non-monotone variant will be used if m_monotone > 1;\nopnorm_maxiter::Int = 5: how many iterations of the power method to use to compute the operator norm of Bₖ. If a negative number is provided, then Arpack is used instead;\nχ::F =  NormLinf(1): norm used to define the trust-region;`\nsubsolver::S = R2Solver: subsolver used to solve the subproblem that appears at each iteration.\nsub_kwargs::NamedTuple = NamedTuple(): a named tuple containing the keyword arguments to be sent to the subsolver. The solver will fail if invalid keyword arguments are provided to the subsolver. For example, if the subsolver is R2Solver, you can pass sub_kwargs = (max_iter = 100, σmin = 1e-6,).\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything other than :unknown will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.TRDH-Union{Tuple{V}, Tuple{T}, Tuple{NLPModelsModifiers.AbstractDiagonalQNModel{T, V}, Any, Any, ROSolverOptions{T}}} where {T, V}","page":"Reference","title":"RegularizedOptimization.TRDH","text":"TRDH(reg_nlp; kwargs…)\nTRDH(nlp, h, χ, options; kwargs...)\nTRDH(f, ∇f!, h, options, x0)\n\nA trust-region method with diagonal Hessian approximation for the problem\n\nmin f(x) + h(x)\n\nwhere f: ℝⁿ → ℝ has a Lipschitz-continuous gradient,, and h: ℝⁿ → ℝ is lower semi-continuous, proper and prox-bounded.\n\nAbout each iterate xₖ, a step sₖ is computed as an approximate solution of\n\nmin  φ(s; xₖ) + ψ(s; xₖ)  subject to  ‖s‖ ≤ Δₖ\n\nwhere φ(s ; xₖ) = f(xₖ) + ∇f(xₖ)ᵀs + ½ sᵀ Dₖ s  is a quadratic approximation of f about xₖ, ψ(s; xₖ) = h(xₖ + s), ‖⋅‖ is a user-defined norm, Dₖ is a diagonal Hessian approximation and Δₖ > 0 is the trust-region radius.\n\nFor advanced usage, first define a solver \"TRDHSolver\" to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TRDH(reg_nlp; D = nothing, χ =  NormLinf(1))\nsolve!(solver, reg_nlp)\n\nstats = RegularizedExecutionStats(reg_nlp)\nsolve!(solver, reg_nlp, stats)\n\nArguments\n\nreg_nlp::AbstractRegularizedNLPModel{T, V}: the problem to solve, see RegularizedProblems.jl, NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess;\natol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nneg_tol::T = eps(T)^(1 / 4): negative tolerance;\nmax_eval::Int = -1: maximum number of evaluation of the objective function (negative number means unlimited);\nmax_time::Float64 = 30.0: maximum time limit in seconds;\nmax_iter::Int = 10000: maximum number of iterations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration;\nΔk::T = T(1): initial value of the trust-region radius;\nη1::T = √√eps(T): successful iteration threshold;\nη2::T = T(0.9): very successful iteration threshold;\nγ::T = T(3): trust-region radius parameter multiplier. Must satisfy γ > 1. The trust-region radius is updated as Δ := Δ*γ when the iteration is very successful and Δ := Δ/γ when the iteration is unsuccessful;\nreduce_TR::Bool = true: see explanation on the stopping criterion below;\nχ::F =  NormLinf(1): norm used to define the trust-region;`\nD::L = nothing: diagonal quasi-Newton approximation used for the model φ. If nothing is provided and reg_nlp.model is not a diagonal quasi-Newton approximation, a spectral gradient approximation is used.`\n\nThe algorithm stops either when √(ξₖ/νₖ) < atol + rtol*√(ξ₀/ν₀) or ξₖ < 0 and √(-ξₖ/νₖ) < neg_tol where ξₖ := f(xₖ) + h(xₖ) - φ(sₖ; xₖ) - ψ(sₖ; xₖ), and √(ξₖ/νₖ) is a stationarity measure. Alternatively, if reduce_TR = true, then ξₖ₁ := f(xₖ) + h(xₖ) - φ(sₖ₁; xₖ) - ψ(sₖ₁; xₖ) is used instead of ξₖ, where sₖ₁ is the Cauchy point.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.xk: current iterate;\nsolver.∇fk: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.solver_specific[:smooth_obj]: current value of the smooth part of the objective function;\nstats.solver_specific[:nonsmooth_obj]: current value of the nonsmooth part of the objective function;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything other than :unknown will stop the algorithm, but you should use :user to properly indicate the intention;\nstats.elapsed_time: elapsed time in seconds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedOptimization.project_y!-Tuple{Percival.AugLagModel}","page":"Reference","title":"RegularizedOptimization.project_y!","text":"project_y!(nlp)\n\nGiven an AugLagModel, project nlp.y into [ymin, ymax] and updates nlp.μc_y accordingly.\n\n\n\n\n\n","category":"method"},{"location":"examples/ls/#A-regularized-nonlinear-least-square-problem","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"","category":"section"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"In this tutorial, we will show how to model and solve the nonconvex nonsmooth least-square problem","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"  min_x in mathbbR^2 tfrac12 sum_i=1^m big(y_i - x_1 e^x_2 t_ibig)^2 + lambda x_0","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"This problem models the fitting of an exponential curve, given noisy data.","category":"page"},{"location":"examples/ls/#Modelling-the-problem","page":"A regularized nonlinear least-square problem","title":"Modelling the problem","text":"","category":"section"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"We first formulate the objective function as the sum of a smooth function f and a nonsmooth regularizer h:","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"  tfrac12 sum_i=1^m big(y_i - x_1 e^x_2 t_ibig)^2 + lambda x_0 = f(x) + h(x)","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"where ","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"beginalign*\nf(x) = tfrac12 sum_i=1^m big(y_i - x_1 e^x_2 t_ibig)^2\nh(x) = lambdax_0\nendalign*","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"To model f, we are going to use ADNLPModels.jl. For the nonsmooth regularizer, we use ProximalOperators.jl.  We then wrap the smooth function and the regularizer in a RegularizedNLPModel.","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"using ADNLPModels\nusing ProximalOperators\nusing Random\nusing RegularizedProblems\n\nRandom.seed!(0)\n\n# Generate synthetic nonlinear least-squares data\nm = 100\nt = range(0, 1, length=m)\na_true, b_true = 2.0, -1.0\ny = [a_true * exp(b_true * ti) + 0.1*randn() for ti in t]\n\n# Starting point\nx0 = [1.0, 0.0]   # [a, b]\n\n# Define nonlinear residuals\nfunction F(x)\n  a, b = x\n  return [yi - a*exp(b*ti) for (ti, yi) in zip(t, y)]\nend\n\n# Build ADNLSModel\nf_model = ADNLSModel(F, x0, m, name = \"nonlinear LS model of f\")\n\n# Get the regularizer from ProximalOperators\nλ  = 0.01\nh = NormL0(λ)\n\n# Wrap into a RegularizedNLPModel\nregularized_pb = RegularizedNLPModel(f_model, h)","category":"page"},{"location":"examples/ls/#Solving-the-problem","page":"A regularized nonlinear least-square problem","title":"Solving the problem","text":"","category":"section"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"We can now choose one of the solvers presented here to solve the problem we defined above. In the case of least-squares, it is usually more appropriate to choose LM or LMTR.","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"using RegularizedOptimization\n\n# LM is a quadratic regularization method.\nout = LM(regularized_pb, verbose = 1, atol = 1e-4)\nprintln(\"LM converged after $(out.iter) iterations.\")","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"We can visualize the solution with plots,","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"using Plots\n\n# Extract estimated parameters\na_est, b_est = out.solution\n\n# True curve\ny_true = [a_true * exp(b_true * ti) for ti in t]\n\n# Estimated curve\ny_est = [a_est * exp(b_est * ti) for ti in t]\n\n# Plot\nscatter(t, y, label=\"Noisy data\", legend=:bottomleft)\nplot!(t, y_true, label=\"True model\", lw=2)\nplot!(t, y_est, label=\"Fitted model\", lw=2, ls=:dash) ","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"#We can choose LMTR instead which is a trust-region method\nout = LMTR(regularized_pb, verbose = 1, atol = 1e-4)\nprintln(\"LMTR converged after $(out.iter) iterations.\")","category":"page"},{"location":"examples/ls/","page":"A regularized nonlinear least-square problem","title":"A regularized nonlinear least-square problem","text":"# Extract estimated parameters\na_est, b_est = out.solution\n\n# Estimated curve\ny_est = [a_est * exp(b_est * ti) for ti in t]\n\n# Plot\nscatter(t, y, label=\"Noisy data\", legend=:bottomleft)\nplot!(t, y_true, label=\"True model\", lw=2)\nplot!(t, y_est, label=\"Fitted model\", lw=2, ls=:dash) \n","category":"page"},{"location":"bibliography/#Bibliography","page":"Bibliography","title":"Bibliography","text":"","category":"section"},{"location":"bibliography/","page":"Bibliography","title":"Bibliography","text":"A. Y. Aravkin, R. Baraldi and D. Orban. A Proximal Quasi-Newton Trust-Region Method for Nonsmooth Regularized Optimization. SIAM Journal on Optimization 32, 900–929 (2022).\n\n\n\nY. Diouane, M. Laghdaf Habiboullah and D. Orban. A proximal modified quasi-Newton method for nonsmooth regularized optimization. Les Cahiers du GERAD G-2024-64 (Groupe d’études et de recherche en analyse des décisions, Montreal, Canada, 2024).\n\n\n\nG. Leconte and D. Orban. The indefinite proximal gradient method. Computational Optimization and Applications 91, 861–903 (2025).\n\n\n\nA. Y. Aravkin, R. Baraldi and D. Orban. A Levenberg–Marquardt Method for Nonsmooth Regularized Least Squares. SIAM Journal on Scientific Computing 46, A2557-A2581 (2024).\n\n\n\n","category":"page"},{"location":"#RegularizedOptimization.jl","page":"Home","title":"RegularizedOptimization.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements a family of algorithms to solve nonsmooth optimization problems of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"undersetx in mathbbR^ntextminimize quad f(x) + h(x) quad textsubject to  c(x) = 0","category":"page"},{"location":"","page":"Home","title":"Home","text":"where f mathbbR^n to mathbbR and c mathbbR^n to mathbbR^m are continuously differentiable, and h mathbbR^n to mathbbR cup +infty is lower semi-continuous. The nonsmooth objective h can be a regularizer such as a sparsity-inducing penalty, model simple constraints such as x belonging to a simple convex set, or be a combination of both. All f, h and c can be nonconvex.","category":"page"},{"location":"","page":"Home","title":"Home","text":"All solvers implemented in this package are JuliaSmoothOptimizers-compliant.   They take a RegularizedNLPModel as input and return a GenericExecutionStats.  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"A RegularizedNLPModel contains:  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"a smooth component f represented as an AbstractNLPModel,  \na nonsmooth regularizer h.  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"For the smooth component f, we refer to jso.dev for tutorials on the NLPModels API. This framework allows the usage of models from  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"AMPL (AmplNLReader.jl),  \nCUTEst (CUTEst.jl),  \nJuMP (NLPModelsJuMP.jl),  \nPDE-constrained problems (PDENLPModels.jl),  \nmodels defined with automatic differentiation (ADNLPModels.jl).","category":"page"},{"location":"","page":"Home","title":"Home","text":"We refer to ManualNLPModels.jl for users interested in defining their own model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For the nonsmooth component h we refer to ShiftedProximalOperators.jl. Regularizers are available in ","category":"page"},{"location":"","page":"Home","title":"Home","text":"ProximalOperators.jl\nShiftedProximalOperators.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#How-to-Install","page":"Home","title":"How to Install","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RegularizedOptimization can be installed through the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]\npkg> add https://github.com/JuliaSmoothOptimizers/RegularizedOptimization.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you think you found a bug, please open an issue.   Focused suggestions and requests can also be opened as issues. Before opening a pull request, we recommend starting an issue or a discussion first.  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"For general questions not suited for a bug report, feel free to start a discussion here.   This forum is for questions and discussions about any of the JuliaSmoothOptimizers packages.  ","category":"page"},{"location":"algorithms/#algorithms","page":"Algorithms","title":"Solvers","text":"","category":"section"},{"location":"algorithms/#General-case","page":"Algorithms","title":"General case","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The solvers in this package are based upon the approach of [1]. Suppose we are given the general regularized problem","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"undersetx in mathbbR^ntextminimize quad f(x) + h(x)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where f  mathbbR^n mapsto mathbbR is continuously differentiable and h  mathbbR^n mapsto mathbbR cup infty is lower semi-continuous. Instead of solving the above directly, which is often impossible, we will solve a simplified version of it repeatedly until we reach a stationary point of the problem above. To do so, suppose we are given an iterate x_0 in mathbbR^n, we wish to compute a step, s_0 in mathbbR^n and improve our iterate with x_1 = x_0 + s_0. Now, we are going to approximate the functions f and h around x_0 with simpler functions (models), which we denote respectively varphi(cdot x_0) and psi(cdot x_0) so that","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"varphi(s x_0) approx f(x_0 + s) quad textand quad psi(s x_0) approx h(x_0 + s) ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"We then wish to compute the step as","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"s_0 in undersets in mathbbR^nargmin   varphi(s x_0) + psi(s x_0)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"In order to ensure convergence and to handle the potential nonconvexity of the objective function, we either add a trust-region constraint,","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"s_0 in undersets in mathbbR^nargmin   varphi(s x_0) + psi(s x_0) quad textsubject to  s leq Delta","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"or a quadratic regularization","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"s_0 in undersets in mathbbR^nargmin   varphi(s x_0) + psi(s x_0) + tfrac12 sigma s^2_2","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Solvers that work with a trust-region are TR and TRDH and the ones working with a quadratic regularization are R2, R2N and R2DH","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The models for the smooth part f in this package are always quadratic models of the form","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"varphi(s x_0) = f(x_0) + nabla f(x_0)^T s + tfrac12 s^T H(x_0) s","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where H(x_0) is a symmetric matrix that can be either 0, the Hessian of f (if it exists) or a quasi-Newton approximation. Some solvers require a specific structure for H, for an overview, refer to the table below.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The following table gives an overview of the available solvers in the general case.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Solver Quadratic Regularization Trust Region Quadratic term for varphi : H Reference\nR2 Yes No H = 0 [1, Algorithm 6.1]\nR2N Yes No Any Symmetric [2, Algorithm 1]\nR2DH Yes No Any Diagonal [2, Algorithm 1]\nTR No Yes Any Symmetric [1, Algorithm 3.1]\nTRDH No Yes Any Diagonal [3, Algorithm 5.1]","category":"page"},{"location":"algorithms/#Nonlinear-least-squares","page":"Algorithms","title":"Nonlinear least-squares","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"This package provides two solvers, LM and LMTR, specialized for regularized, nonlinear least-squares, i.e., problems of the form","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"undersetx in mathbbR^ntextminimize quad tfrac12F(x)_2^2 + h(x)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where F  mathbbR^n mapsto mathbbR^m is continuously differentiable and h  mathbbR^n mapsto mathbbR cup infty is lower semi-continuous. In that case, the model varphi is defined as ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"varphi(s x) = tfrac12F(x) + J(x)s_2^2","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where J(x) is the Jacobian of F at x. Similar to the solvers in the previous section, we either add a quadratic regularization to the model (LM) or a trust-region (LMTR). These solvers are described in [4].","category":"page"},{"location":"algorithms/#Constrained-Optimization","page":"Algorithms","title":"Constrained Optimization","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"For constrained, regularized optimization,","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"undersetx in mathbbR^ntextminimize quad f(x) + h(x) quad textsubject to  l leq x leq u  textand  c(x) = 0","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"an augmented Lagrangian method is provided, AL.","category":"page"}]
}
